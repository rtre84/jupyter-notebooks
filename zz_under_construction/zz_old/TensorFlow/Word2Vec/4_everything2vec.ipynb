{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "everything2vec\n",
    "====\n",
    "\n",
    "<img src=\"images/all_the_things.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "word2vec for Machine Translation\n",
    "---\n",
    "\n",
    "<img src=\"images/machine_translation.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Language translations can be rotations and scalings of the vector space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "How are Machine Translations learned?\n",
    "----\n",
    "\n",
    "The transform matrix can be learned by bootstrapping from a small sample (manually labeled), then extended to entire language.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a word embedding in both languages\n",
    "2. Manually specify pairs (typically, simple concrete nouns)\n",
    "3. Find the translation matrix\n",
    "4. Apply translation matrix across entire language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "word2vec: Review\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary>\n",
    "What is goal of word2vec in Plain English?\n",
    "</summary>\n",
    "<br>\n",
    "Create a dense vector representation of words that models semantic meaning based on context\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<details><summary>\n",
    "Why are neural networks powerful in machine learning?\n",
    "</summary>\n",
    "<br>\n",
    "<br>\n",
    "Capable of learning complex, arbitrary, non-linear relationships.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<details><summary>\n",
    "What are the inputs and outputs of word2vec neural network during training?\n",
    "</summary>\n",
    "<br>\n",
    "<br>\n",
    "word and context\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<details><summary>\n",
    "What math operations are most common for word2vec embedding?\n",
    "</summary>\n",
    "<br>\n",
    "<br>\n",
    "- Arithmetic\n",
    "<br>\n",
    "<br>\n",
    "- Distance\n",
    "<br>\n",
    "<br>\n",
    "- Clustering\n",
    "<br>\n",
    "<br>\n",
    "But any vector operations are meaningful\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Every ML algorithm has to solve 3 basic problems:  \n",
    "\n",
    "1. Representation  \n",
    "2. Evaluation  \n",
    "3. Optimization  \n",
    "\n",
    "<details><summary>\n",
    "How does word2vec solve each of these?\n",
    "</summary>\n",
    "<br>\n",
    "<br>\n",
    "1.Representation of word semantics with dense vectors (key breakthrough)\n",
    "<br>\n",
    "<br>\n",
    "2. Evaluation is through word context (thus needs lots of words)    \n",
    "<br>\n",
    "<br>\n",
    "3. Optimization is backpropagation (aka, back prop)  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "By the end of this Notebook, you should be able to:\n",
    "----\n",
    "\n",
    "- Apply word2vec beyond just words\n",
    "- Explain how doc2vec works to extend word2vec\n",
    "- Explain how word2vec can be applied to other domains (e.g., emojis ðŸ‘»)\n",
    "- Describe the future of vectorization through thought2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "doc2vec, a powerful extension of word2vec\n",
    "====\n",
    "\n",
    "<img src=\"http://img5.picload.org/image/paagccr/doc2vec.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "How does doc2vec work?\n",
    "----\n",
    "\n",
    "Doc2vec (aka paragraph2vec or sentence embeddings) modifies the word2vec algorithm to larger blocks of text, such as sentences, paragraphs or entire documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/overview_word.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"images/overview_paragraph.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Every paragraph is mapped to a unique vector, represented by a column in matrix D and every word is also mapped to a unique vector, represented by a column in matrix W . \n",
    "The paragraph vector and word vectors are averaged or concatenated to predict the next word in a context. \n",
    "\n",
    "Each additional context does not have be a fixed length (because it is vectorized and projected into the same space).\n",
    "\n",
    "Additional parameters but the updates are sparse thus still efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "doc2vec Example: Descri.beer\n",
    "---\n",
    "\n",
    "<img src=\"https://timebusinessblog.files.wordpress.com/2013/03/85632599-e1364519588629.jpg?w=360&h=240&crop=1\" style=\"width: 400px;\"/>\n",
    "\n",
    "How do to make sense of 1.6M beer reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/beer_space.jpg\" style=\"width: 400px;\"/>\n",
    "\n",
    "[Source](http://www.slideshare.net/BenEverson/describeer-demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "emjoi2vec\n",
    "----\n",
    "<img src=\"https://s3.amazonaws.com/instagram-static/engineering-blog/emoji-hashtags/tsne_map_tight.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "[Image](images/https://s3.amazonaws.com/instagram-static/engineering-blog/emoji-hashtags/tsne_map_tight.png)  \n",
    "[Source](http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Notable vectorizations\n",
    "----\n",
    "\n",
    "| Name | Embedding  | \n",
    "|:-------:|:------:| \n",
    "| [Char2Vec](http://arxiv.org/abs/1508.06615) | Character |\n",
    "| [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) | Word | \n",
    "| [GloVe](http://www-nlp.stanford.edu/pubs/glove.pdf) | Word | \n",
    "| [Doc2Vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf) | Sections of text |\n",
    "| [Image2Vec](Image2Vec) | Image |\n",
    "| [Video2Vec](https://www.dropbox.com/s/m99k5md8461xi0s/ICIP_Paper_Revised.pdf) | Video |\n",
    "\n",
    "[Source](http://datascienceassn.org/content/table-xx2vec-algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Thought Vectors - The Future of Vectorizations\n",
    "---\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*KYLrhDHqAAdQaJiN1G4ytA.jpeg\" style=\"width: 400px;\"/>\n",
    "\n",
    "Geoffrey Hinton's, from Google, \"Top Secret\" new algorithm.\n",
    "\n",
    "Instead of embedding words or documents in vector space, embed thoughts in vector space. Their features will represent how each thought relates to other thoughts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It hasn't been publicly released so it is mostly speculation. Keep your eye out for it.\n",
    "\n",
    "> When Google farts ðŸ’¨, the rest of the world ðŸ’©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "----\n",
    "Further study on thought2vec\n",
    "----\n",
    "\n",
    "[Thought2vec teaser](https://wtvox.com/robotics/google-is-working-on-a-new-algorithm-thought-vectors)  \n",
    "[General introduction](http://deeplearning4j.org/thoughtvectors.html)<br>\n",
    "[Skip-Thought Vectors paper](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Summary\n",
    "---\n",
    "\n",
    "- Word2vec is another perspective on Machine Translation through a rotation and translation of embedded space.\n",
    "- Longer pieces of text can also be embedded into the same space as words (i.e., doc2vec).\n",
    "- Given the properties of word2vec (e.g., large corpus input, straightforward training, and vector output), it can be applied to a variety of domains. Including\n",
    "    - emjois\n",
    "    - thoughts\n",
    "    - `<insert your idea here>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Bonus Material\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Document similarity at the concept level \n",
    "---\n",
    "\n",
    "> The Sicilian gelato was extremely rich.\n",
    "\n",
    "vs.\n",
    "\n",
    "> The Italian ice-cream was very velvety.\n",
    "\n",
    "The statements reference the __same__ idea but share __no__ words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Word Moverâ€™s Distance (WMD)\n",
    "----\n",
    "\n",
    "![](images/wmd_illustration_1.png)\n",
    "\n",
    "Represent text documents as a weighted point cloud of embedded words. \n",
    "\n",
    "The distance between two text documents A and B is the minimum cumulative distance that words from document A need to travel to match exactly the point cloud of document B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "Earth moverâ€™s distance metric (EMD)\n",
    "-----\n",
    "\n",
    "Word Moverâ€™s Distance (WMD) is a special case of the [earth moverâ€™s distance metric (EMD)](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)\n",
    "\n",
    "EMD is a method to evaluate dissimilarity between two multi-dimensional distributions in some feature space where a distance measure between single features, which we call the ground distance is given. The EMD 'lifts' this distance from individual features to full distributions.\n",
    "\n",
    "[Deep dive on EMD](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/RUBNER/emd.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Example\n",
    "----\n",
    "\n",
    "![](images/WMD_worked_example.png)\n",
    "\n",
    "State-of-the-art kNN classification accuracy but slowest metric to compute.\n",
    "\n",
    "[Source: From Word Embeddings To Document Distances](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)\n",
    "\n",
    "[Application to Data Science](http://tech.opentable.com/2015/08/11/navigating-themes-in-restaurant-reviews-with-word-movers-distance/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "lda2vec \n",
    "----\n",
    "\n",
    "<img src=\"images/catdog_word2vec_cropped.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Latent Dirichlet allocation (LDA) overview\n",
    "---\n",
    "\n",
    "<img src=\"http://salsahpc.indiana.edu/b649proj/images/proj3_LDA%20structure.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/Acs_esny-qQ/hqdefault.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "lda2vec Executive Summary\n",
    "----\n",
    "\n",
    "<img src=\"images/punchline.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "lda2vec adds additional context. Defines context as a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/doc_vec.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "lda2vec model\n",
    "-----\n",
    "\n",
    "<img src=\"images/lda2vec.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "$v_{doc}$ is a mixture:  \n",
    "$v_{doc}$ = a $v_{topic1}$ + b $v_{topic2}$ + ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "word2vec vs. LDA\n",
    "----\n",
    "\n",
    "<img src=\"images/compare_models.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "| algorithm | scope | prediction | numbers | visualization | density | metaphor| \n",
    "|:-------:|:------:| :------:| :------:| :------:| :------:|  :------:|\n",
    "| word2vec | local | one word predicts a nearby words | real numbers | bar chart | dense | location  |\n",
    "| LDA | global | documents predict global words | percentages that sum to 100%  | pie chart | sparse | mixture| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "lda2vec implementation\n",
    "----\n",
    "\n",
    "[GitHub repo](https://github.com/cemoody/lda2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
