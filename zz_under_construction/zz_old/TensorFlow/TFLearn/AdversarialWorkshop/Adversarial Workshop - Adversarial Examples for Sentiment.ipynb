{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ipolosukhin/projects/tf_examples/.env/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sets logging to INFO to see all information from TensorFlow.\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = 'adversarial_sentiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>the adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>that</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>what</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>good for the goose</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>for</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>is also</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156030</th>\n",
       "      <td>156031</td>\n",
       "      <td>8542</td>\n",
       "      <td>a joke in the United States</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156031</th>\n",
       "      <td>156032</td>\n",
       "      <td>8543</td>\n",
       "      <td>The movie 's downfall is to substitute plot fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156032</th>\n",
       "      <td>156033</td>\n",
       "      <td>8543</td>\n",
       "      <td>The movie 's downfall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156033</th>\n",
       "      <td>156034</td>\n",
       "      <td>8543</td>\n",
       "      <td>is to substitute plot for personality .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156034</th>\n",
       "      <td>156035</td>\n",
       "      <td>8543</td>\n",
       "      <td>is to substitute plot for personality</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156035</th>\n",
       "      <td>156036</td>\n",
       "      <td>8543</td>\n",
       "      <td>to substitute plot for personality</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156036</th>\n",
       "      <td>156037</td>\n",
       "      <td>8543</td>\n",
       "      <td>substitute plot for personality</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156037</th>\n",
       "      <td>156038</td>\n",
       "      <td>8543</td>\n",
       "      <td>substitute plot</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156038</th>\n",
       "      <td>156039</td>\n",
       "      <td>8543</td>\n",
       "      <td>for personality</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156039</th>\n",
       "      <td>156040</td>\n",
       "      <td>8544</td>\n",
       "      <td>The film is darkly atmospheric , with Herrmann...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156040</th>\n",
       "      <td>156041</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156041</th>\n",
       "      <td>156042</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156042</th>\n",
       "      <td>156043</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric ,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156043</th>\n",
       "      <td>156044</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156044</th>\n",
       "      <td>156045</td>\n",
       "      <td>8544</td>\n",
       "      <td>with Herrmann quietly suggesting the sadness a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156045</th>\n",
       "      <td>156046</td>\n",
       "      <td>8544</td>\n",
       "      <td>Herrmann quietly suggesting the sadness and ob...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156046</th>\n",
       "      <td>156047</td>\n",
       "      <td>8544</td>\n",
       "      <td>Herrmann</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156047</th>\n",
       "      <td>156048</td>\n",
       "      <td>8544</td>\n",
       "      <td>quietly suggesting the sadness and obsession b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156048</th>\n",
       "      <td>156049</td>\n",
       "      <td>8544</td>\n",
       "      <td>suggesting the sadness and obsession beneath H...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156049</th>\n",
       "      <td>156050</td>\n",
       "      <td>8544</td>\n",
       "      <td>suggesting the sadness and obsession</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156050</th>\n",
       "      <td>156051</td>\n",
       "      <td>8544</td>\n",
       "      <td>the sadness and obsession</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156051</th>\n",
       "      <td>156052</td>\n",
       "      <td>8544</td>\n",
       "      <td>sadness and obsession</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156052</th>\n",
       "      <td>156053</td>\n",
       "      <td>8544</td>\n",
       "      <td>sadness and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156053</th>\n",
       "      <td>156054</td>\n",
       "      <td>8544</td>\n",
       "      <td>beneath Hearst 's forced avuncular chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156054</th>\n",
       "      <td>156055</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's forced avuncular chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>156056</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>156057</td>\n",
       "      <td>8544</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>156059</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156060 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "0              1           1   \n",
       "1              2           1   \n",
       "2              3           1   \n",
       "3              4           1   \n",
       "4              5           1   \n",
       "5              6           1   \n",
       "6              7           1   \n",
       "7              8           1   \n",
       "8              9           1   \n",
       "9             10           1   \n",
       "10            11           1   \n",
       "11            12           1   \n",
       "12            13           1   \n",
       "13            14           1   \n",
       "14            15           1   \n",
       "15            16           1   \n",
       "16            17           1   \n",
       "17            18           1   \n",
       "18            19           1   \n",
       "19            20           1   \n",
       "20            21           1   \n",
       "21            22           1   \n",
       "22            23           1   \n",
       "23            24           1   \n",
       "24            25           1   \n",
       "25            26           1   \n",
       "26            27           1   \n",
       "27            28           1   \n",
       "28            29           1   \n",
       "29            30           1   \n",
       "...          ...         ...   \n",
       "156030    156031        8542   \n",
       "156031    156032        8543   \n",
       "156032    156033        8543   \n",
       "156033    156034        8543   \n",
       "156034    156035        8543   \n",
       "156035    156036        8543   \n",
       "156036    156037        8543   \n",
       "156037    156038        8543   \n",
       "156038    156039        8543   \n",
       "156039    156040        8544   \n",
       "156040    156041        8544   \n",
       "156041    156042        8544   \n",
       "156042    156043        8544   \n",
       "156043    156044        8544   \n",
       "156044    156045        8544   \n",
       "156045    156046        8544   \n",
       "156046    156047        8544   \n",
       "156047    156048        8544   \n",
       "156048    156049        8544   \n",
       "156049    156050        8544   \n",
       "156050    156051        8544   \n",
       "156051    156052        8544   \n",
       "156052    156053        8544   \n",
       "156053    156054        8544   \n",
       "156054    156055        8544   \n",
       "156055    156056        8544   \n",
       "156056    156057        8544   \n",
       "156057    156058        8544   \n",
       "156058    156059        8544   \n",
       "156059    156060        8544   \n",
       "\n",
       "                                                   Phrase  Sentiment  \n",
       "0       A series of escapades demonstrating the adage ...          1  \n",
       "1       A series of escapades demonstrating the adage ...          2  \n",
       "2                                                A series          2  \n",
       "3                                                       A          2  \n",
       "4                                                  series          2  \n",
       "5       of escapades demonstrating the adage that what...          2  \n",
       "6                                                      of          2  \n",
       "7       escapades demonstrating the adage that what is...          2  \n",
       "8                                               escapades          2  \n",
       "9       demonstrating the adage that what is good for ...          2  \n",
       "10                                demonstrating the adage          2  \n",
       "11                                          demonstrating          2  \n",
       "12                                              the adage          2  \n",
       "13                                                    the          2  \n",
       "14                                                  adage          2  \n",
       "15                        that what is good for the goose          2  \n",
       "16                                                   that          2  \n",
       "17                             what is good for the goose          2  \n",
       "18                                                   what          2  \n",
       "19                                  is good for the goose          2  \n",
       "20                                                     is          2  \n",
       "21                                     good for the goose          3  \n",
       "22                                                   good          3  \n",
       "23                                          for the goose          2  \n",
       "24                                                    for          2  \n",
       "25                                              the goose          2  \n",
       "26                                                  goose          2  \n",
       "27      is also good for the gander , some of which oc...          2  \n",
       "28      is also good for the gander , some of which oc...          2  \n",
       "29                                                is also          2  \n",
       "...                                                   ...        ...  \n",
       "156030                        a joke in the United States          2  \n",
       "156031  The movie 's downfall is to substitute plot fo...          1  \n",
       "156032                              The movie 's downfall          1  \n",
       "156033            is to substitute plot for personality .          1  \n",
       "156034              is to substitute plot for personality          1  \n",
       "156035                 to substitute plot for personality          2  \n",
       "156036                    substitute plot for personality          1  \n",
       "156037                                    substitute plot          2  \n",
       "156038                                    for personality          2  \n",
       "156039  The film is darkly atmospheric , with Herrmann...          2  \n",
       "156040  is darkly atmospheric , with Herrmann quietly ...          2  \n",
       "156041  is darkly atmospheric , with Herrmann quietly ...          2  \n",
       "156042                            is darkly atmospheric ,          2  \n",
       "156043                              is darkly atmospheric          3  \n",
       "156044  with Herrmann quietly suggesting the sadness a...          2  \n",
       "156045  Herrmann quietly suggesting the sadness and ob...          2  \n",
       "156046                                           Herrmann          2  \n",
       "156047  quietly suggesting the sadness and obsession b...          1  \n",
       "156048  suggesting the sadness and obsession beneath H...          2  \n",
       "156049               suggesting the sadness and obsession          2  \n",
       "156050                          the sadness and obsession          2  \n",
       "156051                              sadness and obsession          1  \n",
       "156052                                        sadness and          1  \n",
       "156053        beneath Hearst 's forced avuncular chortles          2  \n",
       "156054                Hearst 's forced avuncular chortles          2  \n",
       "156055                                          Hearst 's          2  \n",
       "156056                          forced avuncular chortles          1  \n",
       "156057                                 avuncular chortles          3  \n",
       "156058                                          avuncular          2  \n",
       "156059                                           chortles          2  \n",
       "\n",
       "[156060 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "train = pandas.read_csv('data/sentiment-train.tsv', sep='\\t')\n",
    "y = train['Sentiment']\n",
    "X = train['Phrase']\n",
    "train_x, test_x, train_y, test_y = learn.estimators._sklearn.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11fed5a50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEACAYAAACpoOGTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntbxAmJlZLS8QZmZW6/8DkbKL1B99pcQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ab0a8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_y.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 18089\n"
     ]
    }
   ],
   "source": [
    "# Preprocess dataset.\n",
    "MAX_DOCUMENT_LENGTH = 15\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length=MAX_DOCUMENT_LENGTH)\n",
    "train_x = np.array(list(vocab_processor.fit_transform(train_x)))\n",
    "test_x = np.array(list(vocab_processor.transform(test_x)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None), Dimension(15)]), is_sparse=False)\n",
      "WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\n",
      "INFO:tensorflow:Create CheckpointSaver\n",
      "INFO:tensorflow:Step 1: loss = 1.60955\n",
      "INFO:tensorflow:Step 101: loss = 1.26434\n",
      "INFO:tensorflow:Step 201: loss = 1.2356\n",
      "INFO:tensorflow:Saving checkpoints for 300 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 301: loss = 1.25715\n",
      "INFO:tensorflow:Step 401: loss = 1.22059\n",
      "INFO:tensorflow:Step 501: loss = 1.34845\n",
      "INFO:tensorflow:Saving checkpoints for 600 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 601: loss = 1.06565\n",
      "INFO:tensorflow:Step 701: loss = 1.21531\n",
      "INFO:tensorflow:Step 801: loss = 1.15981\n",
      "INFO:tensorflow:Saving checkpoints for 900 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 901: loss = 1.27377\n",
      "INFO:tensorflow:Step 1001: loss = 1.23933\n",
      "INFO:tensorflow:Step 1101: loss = 1.17093\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 1201: loss = 1.15219\n",
      "INFO:tensorflow:Step 1301: loss = 1.26084\n",
      "INFO:tensorflow:Step 1401: loss = 1.16772\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 1501: loss = 1.1994\n",
      "INFO:tensorflow:Step 1601: loss = 1.19225\n",
      "INFO:tensorflow:Step 1701: loss = 1.24324\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 1801: loss = 1.20861\n",
      "INFO:tensorflow:Step 1901: loss = 1.23705\n",
      "INFO:tensorflow:Step 2001: loss = 1.19151\n",
      "INFO:tensorflow:Saving checkpoints for 2100 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 2101: loss = 1.18715\n",
      "INFO:tensorflow:Step 2201: loss = 1.08438\n",
      "INFO:tensorflow:Step 2301: loss = 1.23172\n",
      "INFO:tensorflow:Saving checkpoints for 2400 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 2401: loss = 1.10057\n",
      "INFO:tensorflow:Step 2501: loss = 1.11944\n",
      "INFO:tensorflow:Step 2601: loss = 1.0265\n",
      "INFO:tensorflow:Saving checkpoints for 2700 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 2701: loss = 1.23192\n",
      "INFO:tensorflow:Step 2801: loss = 1.10996\n",
      "INFO:tensorflow:Step 2901: loss = 1.06494\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 3001: loss = 1.0529\n",
      "INFO:tensorflow:Step 3101: loss = 1.08353\n",
      "INFO:tensorflow:Step 3201: loss = 1.12154\n",
      "INFO:tensorflow:Saving checkpoints for 3300 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 3301: loss = 1.1033\n",
      "INFO:tensorflow:Step 3401: loss = 1.18604\n",
      "INFO:tensorflow:Step 3501: loss = 1.20706\n",
      "INFO:tensorflow:Saving checkpoints for 3600 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 3601: loss = 1.12039\n",
      "INFO:tensorflow:Step 3701: loss = 1.11305\n",
      "INFO:tensorflow:Step 3801: loss = 1.05329\n",
      "INFO:tensorflow:Saving checkpoints for 3900 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 3901: loss = 1.04252\n",
      "INFO:tensorflow:Step 4001: loss = 1.17652\n",
      "INFO:tensorflow:Step 4101: loss = 1.16211\n",
      "INFO:tensorflow:Saving checkpoints for 4200 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 4201: loss = 0.993466\n",
      "INFO:tensorflow:Step 4301: loss = 1.15043\n",
      "INFO:tensorflow:Step 4401: loss = 0.954472\n",
      "INFO:tensorflow:Saving checkpoints for 4500 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 4501: loss = 1.09003\n",
      "INFO:tensorflow:Step 4601: loss = 1.17354\n",
      "INFO:tensorflow:Step 4701: loss = 0.989598\n",
      "INFO:tensorflow:Saving checkpoints for 4800 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Step 4801: loss = 1.01904\n",
      "INFO:tensorflow:Step 4901: loss = 1.0382\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into adversarial_sentiment/bow/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.05213.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Estimator(params=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "def bow_model(features, target):\n",
    "    target = tf.one_hot(target, 5, 1.0, 0.0)\n",
    "    word_vectors = learn.ops.categorical_variable(\n",
    "        features, n_classes=n_words,\n",
    "        embedding_size=EMBEDDING_SIZE, name='words')\n",
    "    features = tf.reduce_max(word_vectors, reduction_indices=1)\n",
    "    logits = layers.fully_connected(features, 5, activation_fn=None)\n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "    train_op = layers.optimize_loss(\n",
    "        loss, tf.contrib.framework.get_global_step(), learning_rate=0.05, \n",
    "        optimizer='Adagrad')\n",
    "    return tf.argmax(logits, dimension=1), loss, train_op\n",
    "\n",
    "bow_classifier = learn.Estimator(model_fn=bow_model, model_dir=BASE_DIR + 'bow')\n",
    "bow_classifier.fit(x=train_x, y=train_y, steps=5000, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Given features: Tensor(\"input:0\", shape=(?, 15), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None), Dimension(15)]), is_sparse=False).\n",
      "WARNING:tensorflow:Given targets: Tensor(\"output:0\", shape=(?,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False).\n",
      "INFO:tensorflow:Restored model from adversarial_sentiment/bow/model.ckpt-5000-?????-of-00001\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 5000.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for 5000 step: loss = 1.04754, accuracy = 0.587178\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>global_step</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.587178</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.047538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  global_step      loss\n",
       "0  0.587178         5000  1.047538"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame([bow_classifier.evaluate(\n",
    "    x=test_x, y=test_y,\n",
    "    metrics={'accuracy': tf.contrib.metrics.streaming_accuracy})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None), Dimension(15)]), is_sparse=False)\n",
      "WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\n",
      "WARNING:tensorflow:split_squeeze (from tensorflow.contrib.learn.python.learn.ops.array_ops) is deprecated and will be removed after 2016-08-01.\n",
      "Instructions for updating:\n",
      "Please use tf.unpack instead.\n",
      "INFO:tensorflow:Create CheckpointSaver\n",
      "INFO:tensorflow:Step 1: loss = 1.61243\n",
      "INFO:tensorflow:Step 101: loss = 1.26846\n",
      "INFO:tensorflow:Step 201: loss = 1.23199\n",
      "INFO:tensorflow:Saving checkpoints for 300 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 301: loss = 1.23385\n",
      "INFO:tensorflow:Step 401: loss = 1.19263\n",
      "INFO:tensorflow:Step 501: loss = 1.32969\n",
      "INFO:tensorflow:Saving checkpoints for 600 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 601: loss = 1.04041\n",
      "INFO:tensorflow:Step 701: loss = 1.20891\n",
      "INFO:tensorflow:Step 801: loss = 1.14886\n",
      "INFO:tensorflow:Saving checkpoints for 900 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 901: loss = 1.33269\n",
      "INFO:tensorflow:Step 1001: loss = 1.22563\n",
      "INFO:tensorflow:Step 1101: loss = 1.15786\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 1201: loss = 1.148\n",
      "INFO:tensorflow:Step 1301: loss = 1.25627\n",
      "INFO:tensorflow:Step 1401: loss = 1.19172\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 1501: loss = 1.21147\n",
      "INFO:tensorflow:Step 1601: loss = 1.20372\n",
      "INFO:tensorflow:Step 1701: loss = 1.24767\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 1801: loss = 1.19845\n",
      "INFO:tensorflow:Step 1901: loss = 1.23569\n",
      "INFO:tensorflow:Step 2001: loss = 1.13987\n",
      "INFO:tensorflow:Saving checkpoints for 2100 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 2101: loss = 1.12352\n",
      "INFO:tensorflow:Step 2201: loss = 1.00394\n",
      "INFO:tensorflow:Step 2301: loss = 1.28289\n",
      "INFO:tensorflow:Saving checkpoints for 2400 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 2401: loss = 1.08951\n",
      "INFO:tensorflow:Step 2501: loss = 1.01738\n",
      "INFO:tensorflow:Step 2601: loss = 0.965444\n",
      "INFO:tensorflow:Saving checkpoints for 2700 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 2701: loss = 1.18418\n",
      "INFO:tensorflow:Step 2801: loss = 1.10028\n",
      "INFO:tensorflow:Step 2901: loss = 1.09432\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 3001: loss = 1.00068\n",
      "INFO:tensorflow:Step 3101: loss = 1.08538\n",
      "INFO:tensorflow:Step 3201: loss = 1.02842\n",
      "INFO:tensorflow:Saving checkpoints for 3300 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 3301: loss = 0.941588\n",
      "INFO:tensorflow:Step 3401: loss = 1.10664\n",
      "INFO:tensorflow:Step 3501: loss = 1.14334\n",
      "INFO:tensorflow:Saving checkpoints for 3600 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 3601: loss = 0.9974\n",
      "INFO:tensorflow:Step 3701: loss = 1.03089\n",
      "INFO:tensorflow:Step 3801: loss = 1.18575\n",
      "INFO:tensorflow:Saving checkpoints for 3900 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 3901: loss = 0.92216\n",
      "INFO:tensorflow:Step 4001: loss = 1.10689\n",
      "INFO:tensorflow:Step 4101: loss = 1.02194\n",
      "INFO:tensorflow:Saving checkpoints for 4200 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 4201: loss = 0.928252\n",
      "INFO:tensorflow:Step 4301: loss = 1.06694\n",
      "INFO:tensorflow:Step 4401: loss = 0.875008\n",
      "INFO:tensorflow:Saving checkpoints for 4500 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 4501: loss = 0.971746\n",
      "INFO:tensorflow:Step 4601: loss = 1.14544\n",
      "INFO:tensorflow:Step 4701: loss = 1.07033\n",
      "INFO:tensorflow:Saving checkpoints for 4800 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 4801: loss = 0.928354\n",
      "INFO:tensorflow:Step 4901: loss = 0.978073\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into adversarial_sentiment/rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.971083.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Estimator(params=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnn_model(features, target):\n",
    "    target = tf.one_hot(target, 5, 1.0, 0.0)\n",
    "    word_vectors = learn.ops.categorical_variable(\n",
    "        features, n_classes=n_words,\n",
    "        embedding_size=EMBEDDING_SIZE, name='words')\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "    cell = tf.nn.rnn_cell.GRUCell(EMBEDDING_SIZE)\n",
    "    _, encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "    logits = layers.fully_connected(encoding, 5, activation_fn=None)\n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "    train_op = layers.optimize_loss(\n",
    "        loss, tf.contrib.framework.get_global_step(), learning_rate=0.05, \n",
    "        optimizer='Adagrad')\n",
    "    return tf.argmax(logits, dimension=1), loss, train_op\n",
    "\n",
    "rnn_classifier = learn.Estimator(model_fn=rnn_model, model_dir=BASE_DIR + 'rnn')\n",
    "rnn_classifier.fit(x=train_x, y=train_y, steps=5000, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Given features: Tensor(\"input:0\", shape=(?, 15), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None), Dimension(15)]), is_sparse=False).\n",
      "WARNING:tensorflow:Given targets: Tensor(\"output:0\", shape=(?,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False).\n",
      "WARNING:tensorflow:split_squeeze (from tensorflow.contrib.learn.python.learn.ops.array_ops) is deprecated and will be removed after 2016-08-01.\n",
      "Instructions for updating:\n",
      "Please use tf.unpack instead.\n",
      "INFO:tensorflow:Restored model from adversarial_sentiment/rnn/model.ckpt-5000-?????-of-00001\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 5000.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for 5000 step: loss = 0.972781, accuracy = 0.611399\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>global_step</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.611399</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.972781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  global_step      loss\n",
       "0  0.611399         5000  0.972781"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame([rnn_classifier.evaluate(\n",
    "    x=test_x, y=test_y,\n",
    "    metrics={'accuracy': tf.contrib.metrics.streaming_accuracy})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create adversarial examples.\n",
    "\n",
    "def get_adversarial_example(features, loss, norm):\n",
    "    gradient = tf.gradients(loss, features, aggregation_method=2)\n",
    "    gradient = tf.stop_gradient([g for g in gradient if g is not None])\n",
    "    r_adv = norm * tf.nn.l2_normalize(gradient, dim=1)\n",
    "    r_adv = tf.reduce_sum(r_adv, reduction_indices=[0])\n",
    "    return features + r_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None), Dimension(15)]), is_sparse=False)\n",
      "WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\n",
      "INFO:tensorflow:Create CheckpointSaver\n",
      "INFO:tensorflow:Restored model from adversarial_sentiment/adv_rnn/model.ckpt-5000-?????-of-00001\n",
      "INFO:tensorflow:Step 5001: loss = 1.742\n",
      "INFO:tensorflow:Saving checkpoints for 5001 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 5101: loss = 1.82385\n",
      "INFO:tensorflow:Step 5201: loss = 1.70035\n",
      "INFO:tensorflow:Step 5301: loss = 1.71686\n",
      "INFO:tensorflow:Saving checkpoints for 5301 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 5401: loss = 1.67291\n",
      "INFO:tensorflow:Step 5501: loss = 1.87845\n",
      "INFO:tensorflow:Step 5601: loss = 1.40581\n",
      "INFO:tensorflow:Saving checkpoints for 5601 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 5701: loss = 1.70321\n",
      "INFO:tensorflow:Step 5801: loss = 1.62831\n",
      "INFO:tensorflow:Step 5901: loss = 1.89951\n",
      "INFO:tensorflow:Saving checkpoints for 5901 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 6001: loss = 1.7182\n",
      "INFO:tensorflow:Step 6101: loss = 1.72493\n",
      "INFO:tensorflow:Step 6201: loss = 1.63271\n",
      "INFO:tensorflow:Saving checkpoints for 6201 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 6301: loss = 1.83843\n",
      "INFO:tensorflow:Step 6401: loss = 1.77243\n",
      "INFO:tensorflow:Step 6501: loss = 1.75109\n",
      "INFO:tensorflow:Saving checkpoints for 6501 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 6601: loss = 1.77087\n",
      "INFO:tensorflow:Step 6701: loss = 1.75966\n",
      "INFO:tensorflow:Step 6801: loss = 1.69533\n",
      "INFO:tensorflow:Saving checkpoints for 6801 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 6901: loss = 1.77419\n",
      "INFO:tensorflow:Step 7001: loss = 1.65683\n",
      "INFO:tensorflow:Step 7101: loss = 1.57008\n",
      "INFO:tensorflow:Saving checkpoints for 7101 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 7201: loss = 1.37181\n",
      "INFO:tensorflow:Step 7301: loss = 1.76028\n",
      "INFO:tensorflow:Step 7401: loss = 1.56617\n",
      "INFO:tensorflow:Saving checkpoints for 7401 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 7501: loss = 1.46587\n",
      "INFO:tensorflow:Step 7601: loss = 1.41699\n",
      "INFO:tensorflow:Step 7701: loss = 1.73248\n",
      "INFO:tensorflow:Saving checkpoints for 7701 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 7801: loss = 1.63238\n",
      "INFO:tensorflow:Step 7901: loss = 1.6286\n",
      "INFO:tensorflow:Step 8001: loss = 1.50539\n",
      "INFO:tensorflow:Saving checkpoints for 8001 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 8101: loss = 1.60305\n",
      "INFO:tensorflow:Step 8201: loss = 1.5547\n",
      "INFO:tensorflow:Step 8301: loss = 1.44277\n",
      "INFO:tensorflow:Saving checkpoints for 8301 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 8401: loss = 1.73944\n",
      "INFO:tensorflow:Step 8501: loss = 1.68894\n",
      "INFO:tensorflow:Step 8601: loss = 1.55223\n",
      "INFO:tensorflow:Saving checkpoints for 8601 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 8701: loss = 1.66006\n",
      "INFO:tensorflow:Step 8801: loss = 1.56158\n",
      "INFO:tensorflow:Step 8901: loss = 1.50359\n",
      "INFO:tensorflow:Saving checkpoints for 8901 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 9001: loss = 1.75644\n",
      "INFO:tensorflow:Step 9101: loss = 1.70417\n",
      "INFO:tensorflow:Step 9201: loss = 1.48281\n",
      "INFO:tensorflow:Saving checkpoints for 9201 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 9301: loss = 1.72135\n",
      "INFO:tensorflow:Step 9401: loss = 1.44354\n",
      "INFO:tensorflow:Step 9501: loss = 1.64844\n",
      "INFO:tensorflow:Saving checkpoints for 9501 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 9601: loss = 1.82438\n",
      "INFO:tensorflow:Step 9701: loss = 1.56179\n",
      "INFO:tensorflow:Step 9801: loss = 1.60189\n",
      "INFO:tensorflow:Saving checkpoints for 9801 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Step 9901: loss = 1.58543\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.52567.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Estimator(params=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADV_LOSS_WEIGHT = 0.5\n",
    "\n",
    "def adv_rnn_model(features, target):\n",
    "    features = tf.identity(features, name='features')\n",
    "    target = tf.one_hot(target, 5, 1.0, 0.0)\n",
    "    # Convert features to word vectors.\n",
    "    features = learn.ops.categorical_variable(\n",
    "        features, n_classes=n_words,\n",
    "        embedding_size=EMBEDDING_SIZE, name='words')\n",
    "    def model(features):\n",
    "        word_list = tf.unpack(features, axis=1)\n",
    "        cell = tf.nn.rnn_cell.GRUCell(EMBEDDING_SIZE)\n",
    "        _, encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "        logits = layers.fully_connected(encoding, 5, activation_fn=None)\n",
    "        loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "        return logits, loss\n",
    "    with tf.variable_scope('model'):\n",
    "        logits, orig_loss = model(features)\n",
    "    adv_features = tf.identity(get_adversarial_example(features=features, loss=orig_loss, norm=0.1), name='adv_features')\n",
    "    tf.identity(adv_features - features, name='adv_diff')\n",
    "    with tf.variable_scope('model', reuse=True):\n",
    "        adv_logit, adv_loss = model(adv_features)\n",
    "    adv_pred = tf.argmax(adv_logit, dimension=1, name='adv_prediction')\n",
    "    loss = orig_loss + ADV_LOSS_WEIGHT * adv_loss\n",
    "    train_op = layers.optimize_loss(\n",
    "        loss, tf.contrib.framework.get_global_step(), learning_rate=0.05, \n",
    "        optimizer='Adagrad')\n",
    "    return tf.argmax(logits, dimension=1), loss, train_op\n",
    "\n",
    "adversarial_rnn_classifier = learn.Estimator(model_fn=adv_rnn_model, model_dir=BASE_DIR + 'adv_rnn')\n",
    "adversarial_rnn_classifier.fit(x=train_x, y=train_y, steps=5000, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Given features: Tensor(\"input:0\", shape=(?, 15), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None), Dimension(15)]), is_sparse=False).\n",
      "WARNING:tensorflow:Given targets: Tensor(\"output:0\", shape=(?,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False).\n",
      "INFO:tensorflow:Create CheckpointSaver\n",
      "INFO:tensorflow:Restored model from adversarial_sentiment/adv_rnn/model.ckpt-10000-?????-of-00001\n",
      "INFO:tensorflow:Step 10001: adv_features = [[[-0.09856126 -0.09312834 -0.08946314  0.0410028   0.10420589  0.10681324\n",
      "    0.09001259  0.08756039  0.0987272  -0.09019977 -0.11129735 -0.10305759\n",
      "   -0.0890535  -0.09727825 -0.09626564  0.10022548  0.09829236  0.10965458\n",
      "   -0.07882576  0.09821449 -0.10032161 -0.09028951  0.10962603 -0.09472139\n",
      "   -0.10885916  0.09345677 -0.10487392 -0.10754646 -0.10600623  0.08885272\n",
      "    0.08677614 -0.1087176  -0.09662693  0.10833728  0.11023705  0.10761855\n",
      "   -0.15223123 -0.11148477  0.09590916 -0.10605043  0.09522453 -0.1092566\n",
      "   -0.0983576   0.10119615 -0.10699889  0.10892528 -0.0907819   0.10463062\n",
      "   -0.10854153  0.10294732]\n",
      "  [-0.11224194 -0.08907159 -0.10569058  0.0692647  -0.09804207 -0.08771527\n",
      "    0.10155439  0.10553519  0.09027489 -0.09295928  0.10048214 -0.0898969\n",
      "   -0.10612247  0.09854729 -0.11076172  0.11103655  0.08912621  0.09113497\n",
      "   -0.08830783 -0.09292367 -0.09833958 -0.10531826  0.09481569  0.09947857\n",
      "   -0.10926936  0.10588522 -0.09178397 -0.08903605 -0.10124587  0.10264377\n",
      "   -0.09624425 -0.09021521 -0.09912341  0.09733263  0.10428049  0.08948101\n",
      "   -0.1032951  -0.10461458 -0.09143528  0.10619193  0.10974631 -0.08753882\n",
      "   -0.10251041  0.09344835 -0.0893226   0.11237746 -0.09072986 -0.10798591\n",
      "   -0.09625977  0.09966493]\n",
      "  [ 0.09327658  0.09391818 -0.11091737  0.26579881 -0.10078906 -0.11032292\n",
      "    0.1083744  -0.11045393 -0.09154586  0.08890308  0.08848669 -0.10679863\n",
      "    0.10153676  0.10906561 -0.09787792 -0.10226455 -0.09209383 -0.09481281\n",
      "    0.10521026 -0.09030739 -0.09958737 -0.10040085 -0.09272109 -0.09175505\n",
      "   -0.10311904  0.11104991  0.10380865 -0.08973558 -0.1090145   0.09089009\n",
      "    0.09879857 -0.09217477 -0.09616119 -0.10801519  0.10245851 -0.10700481\n",
      "   -0.13024171 -0.10273942  0.09261742 -0.10935834  0.10236743 -0.11065852\n",
      "    0.09209     0.11030422 -0.10482085 -0.11122623 -0.09671956 -0.09707483\n",
      "   -0.08676051  0.10744526]\n",
      "  [-0.04144292 -0.74775136 -0.04706799 -0.02622526 -0.09702299  0.11687165\n",
      "    0.04596546  0.44890973 -0.06250727 -0.05474911 -0.09241221  0.11046814\n",
      "    0.10307513 -0.19089556 -0.51757586 -0.110677    0.06876913  0.08533161\n",
      "   -1.0323441  -0.10233774 -0.05865537 -0.27746406 -0.10849052  0.11200659\n",
      "    0.12140909 -0.16554295 -0.31183469  0.24340966  0.20773447  0.0545701\n",
      "   -0.12103784 -0.08072543  0.17717975  0.09666895 -0.11718345  0.10883982\n",
      "    0.59910935 -0.08265504  0.09838404 -0.11201432  0.04997603  0.11540036\n",
      "   -0.31853884  0.13294673 -0.06268696  0.08089519 -0.05912825  0.11138345\n",
      "    0.10863503  0.02802933]\n",
      "  [-0.04144292 -0.74775136 -0.04706799 -0.02622529 -0.097023    0.11687164\n",
      "    0.04596546  0.44890973 -0.06250726 -0.05474913 -0.09241222  0.11046814\n",
      "   -0.09692489 -0.19089556 -0.51757586 -0.110677    0.06876913  0.08533162\n",
      "   -1.0323441   0.09766225 -0.05865536 -0.27746403 -0.1084905  -0.0879934\n",
      "    0.12140909 -0.16554293 -0.31183472  0.24340966  0.20773448  0.05457009\n",
      "   -0.12103784 -0.08072545  0.17717975  0.09666896 -0.11718345  0.10883984\n",
      "    0.59910935 -0.08265506  0.09838404 -0.11201434  0.04997603  0.11540035\n",
      "   -0.31853884  0.13294674  0.13731304  0.08089517 -0.05912825 -0.08861656\n",
      "    0.10863503  0.02802932]\n",
      "  [ 0.15855709 -0.74775136  0.152932   -0.02622528 -0.097023    0.11687165\n",
      "   -0.15403454  0.44890973 -0.06250726 -0.05474911 -0.09241223  0.11046813\n",
      "   -0.09692487 -0.19089556 -0.51757586 -0.110677    0.06876913  0.08533162\n",
      "   -1.0323441   0.09766225 -0.05865536 -0.27746406  0.09150948 -0.08799341\n",
      "    0.1214091  -0.16554293 -0.31183472  0.24340966  0.20773447  0.0545701\n",
      "   -0.12103784 -0.08072542  0.17717975  0.09666896 -0.11718345  0.10883981\n",
      "    0.59910935  0.11734498  0.09838403 -0.11201432  0.04997602  0.11540036\n",
      "   -0.31853884  0.13294673  0.13731304  0.08089519 -0.05912825 -0.08861656\n",
      "    0.10863502  0.02802932]\n",
      "  [ 0.15855709 -0.74775136  0.152932   -0.02622528 -0.097023    0.11687166\n",
      "   -0.15403454  0.44890973 -0.06250726  0.14525089  0.1075878   0.11046813\n",
      "   -0.09692487 -0.19089554 -0.51757586 -0.11067702  0.06876911  0.08533162\n",
      "   -1.0323441   0.09766227  0.14134464 -0.27746406 -0.10849052 -0.08799341\n",
      "    0.12140909 -0.16554293 -0.31183469  0.24340966  0.20773448  0.0545701\n",
      "    0.07896216  0.11927457  0.17717975 -0.10333104 -0.11718346 -0.09116016\n",
      "    0.59910941  0.11734496  0.09838402 -0.11201432  0.04997603  0.11540037\n",
      "   -0.31853884 -0.06705327  0.13731304 -0.11910481 -0.05912825 -0.08861656\n",
      "    0.10863503 -0.17197067]\n",
      "  [ 0.15855709 -0.74775136  0.152932   -0.02622528 -0.097023    0.11687165\n",
      "   -0.15403454  0.44890973 -0.06250726  0.14525089  0.10758778  0.11046813\n",
      "   -0.09692486 -0.19089554 -0.51757586 -0.110677    0.06876911  0.08533162\n",
      "   -1.0323441   0.09766226  0.14134464 -0.27746406 -0.10849051 -0.0879934\n",
      "   -0.07859091 -0.16554293 -0.31183472  0.24340965  0.20773447  0.0545701\n",
      "    0.07896218  0.11927457  0.17717975 -0.10333104 -0.11718345 -0.09116018\n",
      "    0.59910935  0.11734496  0.09838403 -0.11201432  0.04997602  0.11540036\n",
      "   -0.31853887 -0.06705326  0.13731307 -0.11910481 -0.05912825 -0.08861656\n",
      "   -0.09136497 -0.17197067]\n",
      "  [ 0.15855709 -0.74775136  0.15293202 -0.02622526 -0.097023    0.11687165\n",
      "   -0.15403454  0.44890973  0.13749276  0.14525089  0.10758778  0.11046813\n",
      "   -0.09692489 -0.19089554 -0.51757586 -0.110677   -0.13123088  0.08533162\n",
      "   -1.0323441   0.09766226  0.14134464 -0.27746403 -0.1084905  -0.08799341\n",
      "   -0.07859091  0.03445707 -0.31183472  0.24340966  0.20773448  0.05457009\n",
      "   -0.12103784  0.11927457  0.17717975 -0.10333104 -0.11718346 -0.09116018\n",
      "    0.59910935  0.11734495  0.09838404 -0.11201432  0.04997603  0.11540036\n",
      "   -0.11853886 -0.06705327  0.13731304 -0.11910482  0.14087175 -0.08861658\n",
      "   -0.09136496 -0.17197067]\n",
      "  [ 0.15855709 -0.74775136  0.15293202 -0.02622528 -0.097023    0.11687165\n",
      "   -0.15403454  0.44890973  0.13749275  0.14525089  0.10758778 -0.08953185\n",
      "   -0.09692487 -0.19089556 -0.31757584 -0.110677   -0.13123086  0.08533162\n",
      "   -1.0323441   0.09766226  0.14134464 -0.27746403 -0.10849052 -0.08799341\n",
      "   -0.07859091  0.03445707 -0.31183469  0.24340966  0.20773447  0.05457009\n",
      "   -0.12103784  0.11927457  0.17717975  0.09666895 -0.11718345 -0.09116019\n",
      "    0.59910941  0.11734496  0.09838404 -0.11201432  0.04997602  0.11540036\n",
      "   -0.11853887 -0.06705326  0.13731304 -0.11910482  0.14087175 -0.08861656\n",
      "   -0.09136497 -0.17197067]\n",
      "  [ 0.15855709 -0.54775131  0.152932   -0.02622528 -0.097023    0.11687164\n",
      "   -0.15403453  0.44890973  0.13749273  0.14525089  0.10758778 -0.08953185\n",
      "   -0.09692487 -0.19089556 -0.31757584 -0.110677   -0.13123086  0.08533161\n",
      "   -1.0323441   0.09766224  0.14134464 -0.27746403 -0.10849052 -0.08799341\n",
      "   -0.07859091  0.03445707 -0.31183472  0.24340966  0.20773448  0.0545701\n",
      "   -0.12103783  0.11927457  0.17717975  0.09666896 -0.11718346 -0.09116018\n",
      "    0.59910935  0.11734496 -0.10161597 -0.11201432  0.04997603 -0.08459964\n",
      "   -0.11853887 -0.06705327  0.13731304 -0.11910482  0.14087173 -0.08861656\n",
      "    0.10863503  0.02802934]\n",
      "  [ 0.1585571  -0.54775131  0.152932   -0.02622526 -0.097023    0.11687164\n",
      "   -0.15403453  0.44890973  0.13749275 -0.05474911  0.10758778 -0.08953186\n",
      "    0.10307513 -0.19089556 -0.31757584 -0.110677   -0.13123086  0.08533161\n",
      "   -1.0323441  -0.10233775  0.14134464 -0.27746406 -0.10849052 -0.08799339\n",
      "   -0.07859091  0.03445707 -0.11183471  0.04340966  0.20773448  0.0545701\n",
      "   -0.12103784  0.11927457  0.17717975  0.09666896 -0.11718345 -0.09116018\n",
      "    0.59910941  0.11734496 -0.10161596 -0.11201432  0.04997603 -0.08459964\n",
      "   -0.31853887 -0.06705326  0.13731304  0.08089519  0.14087173 -0.08861656\n",
      "    0.00481079  0.02802933]\n",
      "  [ 0.15855709 -0.54775131  0.15293202 -0.02622528 -0.097023   -0.08312836\n",
      "   -0.15403454  0.44890973  0.13749275 -0.05474911  0.10758778 -0.08953185\n",
      "    0.10307512 -0.19089554 -0.31757584 -0.110677   -0.13123088  0.08533162\n",
      "   -0.83234406 -0.10233774  0.14134464 -0.27746406 -0.10849051 -0.0879934\n",
      "   -0.0785909   0.03445707 -0.11183472  0.04340966  0.00773448  0.05457012\n",
      "   -0.12103783  0.11927457  0.17717975  0.09666896 -0.11718345 -0.09116018\n",
      "    0.59910941  0.11734496 -0.10161596 -0.11201432  0.04997602 -0.08459964\n",
      "   -0.31853884 -0.06705327  0.13731304  0.0808952   0.14087173  0.11138344\n",
      "   -0.09136496  0.02802933]\n",
      "  [ 0.1585571  -0.54775131  0.15293202 -0.02622528 -0.097023    0.11687165\n",
      "    0.04596546  0.44890973  0.13749275 -0.05474911  0.10758778 -0.08953185\n",
      "    0.10307512 -0.19089553 -0.51757586 -0.110677    0.06876914 -0.11466838\n",
      "   -0.83234406 -0.10233775  0.14134464 -0.27746403 -0.10849052 -0.08799341\n",
      "   -0.07859091  0.03445707 -0.1118347   0.04340966  0.00773448  0.0545701\n",
      "   -0.12103784 -0.08072545  0.17717975  0.09666896 -0.11718346  0.10883981\n",
      "    0.59910935  0.11734496 -0.10161596 -0.11201432  0.04997603 -0.08459964\n",
      "   -0.31853887 -0.06705327  0.13731304  0.08089519  0.14087173  0.11138343\n",
      "   -0.09136497  0.02802932]\n",
      "  [-0.04144292 -0.54775131 -0.04706798 -0.02622526 -0.097023    0.11687165\n",
      "    0.04596546  0.44890973 -0.06250727 -0.05474911  0.10758778 -0.08953188\n",
      "    0.10307512 -0.19089556 -0.51757586 -0.110677    0.06876913 -0.11466838\n",
      "   -0.83234406 -0.10233775  0.14134464 -0.27746406 -0.10849051 -0.0879934\n",
      "    0.12140909  0.03445707 -0.11183472  0.04340965  0.00773448  0.0545701\n",
      "   -0.12103784 -0.08072542  0.17717975  0.09666896 -0.11718346  0.10883982\n",
      "    0.59910941  0.11734495  0.09838404 -0.11201432  0.04997603 -0.08459964\n",
      "   -0.31853887 -0.06705327 -0.06268695  0.08089518  0.14087173  0.11138343\n",
      "   -0.09136497  0.02802932]]], adv_prediction = [3], features = [[  722 12109  2027     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]]\n",
      "INFO:tensorflow:Step 10001: loss = 1.94626\n",
      "INFO:tensorflow:Saving checkpoints for 10001 into adversarial_sentiment/adv_rnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.94626.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Estimator(params=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial_rnn_classifier.fit(\n",
    "    x=train_x, y=train_y, steps=1, batch_size=1,\n",
    "    monitors=[learn.monitors.PrintTensor(['adv_prediction', 'features', 'adv_features'], every_n=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Given features: Tensor(\"input:0\", shape=(?, 15), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None), Dimension(15)]), is_sparse=False).\n",
      "WARNING:tensorflow:Given targets: Tensor(\"output:0\", shape=(?,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False).\n",
      "INFO:tensorflow:Restored model from adversarial_sentiment/adv_rnn/model.ckpt-10000-?????-of-00001\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 10000.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for 10000 step: loss = 1.54573, accuracy = 0.592048\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>global_step</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.592048</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.545733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  global_step      loss\n",
       "0  0.592048        10000  1.545733"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame([adversarial_rnn_classifier.evaluate(\n",
    "    x=test_x, y=test_y,\n",
    "    metrics={'accuracy': tf.contrib.metrics.streaming_accuracy})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
