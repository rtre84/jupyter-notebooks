{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory='mnist'\n",
    "train_data_filename = directory+'/train-images-idx3-ubyte.gz'\n",
    "train_labels_filename = directory+'/train-labels-idx1-ubyte.gz'\n",
    "test_data_filename = directory+'/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_filename =directory+'/t10k-labels-idx1-ubyte.gz'\n",
    "print test_data_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data, show the first data point f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip, binascii, struct, numpy\n",
    "\n",
    "with gzip.open(test_data_filename) as f:\n",
    "  for field in ['magic number', 'image count', 'rows', 'columns']:\n",
    "    line=f.read(4)  #read the first 4 elements in current row\n",
    "    item0=struct.unpack('>i', line)[0]  #unpack binary data and interpret as integer\n",
    "    print field, item0\n",
    "    \n",
    "  buf = f.read(28 * 28)  #read 28x28=784 rows\n",
    "  #image is a one-dimensional array of 28x28=784 rows elements\n",
    "  image = numpy.frombuffer(buf, dtype=numpy.uint8) #data type is integer\n",
    "\n",
    "  print \"image is:\"\n",
    "  print image\n",
    "  print 'The first 10 pixels of image', image[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll show the image and its pixel value histogram side-by-side.\n",
    "plot1= plt.subplot(1, 3, 1) #the plot has 1 row, 2 columns\n",
    "plot2= plt.subplot(1, 3, 2)\n",
    "plot3= plt.subplot(1, 3, 3)\n",
    "\n",
    "plot1.imshow(image.reshape(28, 28));  #reshape the image to 28x28 pixels\n",
    "plot2.imshow(image.reshape(28, 28), cmap=plt.cm.Greys);  #with grey scale\n",
    "plot3.hist(image, bins=20, range=[0,255]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the data label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open(test_labels_filename) as f:\n",
    "  for field in ['magic number', 'label count']:\n",
    "    line=f.read(4)  #read the first 4 elements in current row\n",
    "    item0=struct.unpack('>i', line)[0]\n",
    "    print field, item0\n",
    "\n",
    "  line= f.read(1) #red the next line\n",
    "  print 'First label:', struct.unpack('B',line)[0]  #unpacked binary data, and interpret as 'B'=unsigned char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data, as 4-d tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "PIXEL_DEPTH = 255\n",
    "\n",
    "\"\"\"\n",
    "  Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "  For greyscale MNIST, the number of channels is always 1.\n",
    "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "==================================================================\n",
    "\"\"\"\n",
    "def extract_data(filename, num_images):\n",
    "  print 'Extracting', filename\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    # Skip the magic number and dimensions; we know these values.\n",
    "    bytestream.read(16)\n",
    "    \n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
    "    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "    return data\n",
    "\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Training data shape', train_data.shape\n",
    "plot1= plt.subplot(1, 2, 1) #the plot has 1 row, 2 columns\n",
    "plot2= plt.subplot(1, 2, 2)\n",
    "\n",
    "image1=train_data[0].reshape(28, 28)\n",
    "image2=train_data[1].reshape(28, 28)\n",
    "plot1.imshow(image1, cmap=plt.cm.Greys);\n",
    "plot2.imshow(image2, cmap=plt.cm.Greys);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_LABELS = 10\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "  print 'Extracting', filename\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    # Skip the magic number and count; we know these values.\n",
    "    bytestream.read(8)\n",
    "    \n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "  # Convert to dense 1-hot representation.\n",
    "  return (numpy.arange(NUM_LABELS) == labels[:, None]).astype(numpy.float32)\n",
    "\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)\n",
    "print 'Training labels shape', train_labels.shape\n",
    "print 'First label vector', train_labels[0]\n",
    "print 'Second label vector', train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the training data into 2 sets: training , validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 5000\n",
    "\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "print 'Validation shape', validation_data.shape\n",
    "print 'Train size', train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 60 #Each batch is the number of data we feed into NN each time\n",
    "# We have only one channel in our grayscale images.\n",
    "NUM_CHANNELS = 1\n",
    "# The random seed that defines initialization.\n",
    "SEED = 42\n",
    "\n",
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step, which we'll write once we define the graph structure.\n",
    "train_data_node = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,shape=(BATCH_SIZE, NUM_LABELS))\n",
    "\n",
    "# For the validation and test data, we'll just hold the entire dataset in one constant node.\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The variables below hold all the trainable weights. For each, the\n",
    "# parameter defines how the variables will be initialized.\n",
    "conv1_weights = tf.Variable(tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "\n",
    "conv2_weights = tf.Variable(tf.truncated_normal([5, 5, 32, 64],stddev=0.1,seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "# fully connected, depth 512.\n",
    "fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE / 4 * IMAGE_SIZE / 4 * 64, 512],stddev=0.1,seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],stddev=0.1, seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(data, train=False):\n",
    "  \"\"\"The Model definition.\"\"\"\n",
    "  # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "  # the same size as the input). Note that {strides} is a 4D array whose\n",
    "  # shape matches the data layout: [image index, y, x, depth].\n",
    "  conv = tf.nn.conv2d(data,\n",
    "                      conv1_weights,\n",
    "                      strides=[1, 1, 1, 1],\n",
    "                      padding='SAME')\n",
    "\n",
    "  # Bias and rectified linear non-linearity.\n",
    "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "  \n",
    "  # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "  # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "  pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "  conv = tf.nn.conv2d(pool,\n",
    "                      conv2_weights,\n",
    "                      strides=[1, 1, 1, 1],\n",
    "                      padding='SAME')\n",
    "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "  pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "  \n",
    "  # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "  # fully connected layers.\n",
    "  pool_shape = pool.get_shape().as_list()\n",
    "  reshape = tf.reshape(\n",
    "      pool,\n",
    "      [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "  # Fully connected layer. Note that the '+' operation automatically\n",
    "  # broadcasts the biases.\n",
    "  hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "  \n",
    "  # Add a 50% dropout during training only. Dropout also scales\n",
    "  # activations such that no rescaling is needed at evaluation time.\n",
    "  if train:\n",
    "    hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "  return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  logits, train_labels_node))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new interactive session that we'll use in\n",
    "# subsequent code cells.\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "# Use our newly created session as the default for \n",
    "# subsequent operations.\n",
    "s.as_default()\n",
    "\n",
    "# Initialize all the variables we defined above.\n",
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "\n",
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = s.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The highest probability in the first entry.\n",
    "print 'First prediction', numpy.argmax(predictions[0])\n",
    "print predictions.shape\n",
    "print 'All predictions', numpy.argmax(predictions, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Batch labels', numpy.argmax(batch_labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(batch_labels, 1))\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print float(correct) / float(total)\n",
    "\n",
    "confusions = numpy.zeros([10, 10], numpy.float32)\n",
    "bundled = zip(numpy.argmax(predictions, 1), numpy.argmax(batch_labels, 1))\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(numpy.arange(NUM_LABELS))\n",
    "plt.yticks(numpy.arange(NUM_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "  \"\"\"Return the error rate and confusions.\"\"\"\n",
    "  correct = numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(labels, 1))\n",
    "  total = predictions.shape[0]\n",
    "\n",
    "  error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "  confusions = numpy.zeros([10, 10], numpy.float32)\n",
    "  bundled = zip(numpy.argmax(predictions, 1), numpy.argmax(labels, 1))\n",
    "  for predicted, actual in bundled:\n",
    "    confusions[predicted, actual] += 1\n",
    "    \n",
    "  return error, confusions\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train over the first 1/4th of our training set.\n",
    "steps = int(train_size / BATCH_SIZE)\n",
    "for step in xrange(steps):\n",
    "  # Compute the offset of the current minibatch in the data.\n",
    "  # Note that we could use better randomization across epochs.\n",
    "  offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "  batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "  batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "  # This dictionary maps the batch data (as a numpy array) to the\n",
    "  # node in the graph it should be fed to.\n",
    "  feed_dict = {train_data_node: batch_data,\n",
    "               train_labels_node: batch_labels}\n",
    "  # Run the graph and fetch some of the nodes.\n",
    "  _, l, lr, predictions = s.run(\n",
    "    [optimizer, loss, learning_rate, train_prediction],\n",
    "    feed_dict=feed_dict)\n",
    "  \n",
    "  # Print out the loss periodically.\n",
    "  if step % 100 == 0:\n",
    "    error, _ = error_rate(predictions, batch_labels)\n",
    "    print 'Step %d of %d' % (step, steps)\n",
    "    print 'Mini-batch loss: %.5f Error: %.5f Learning rate: %.5f' % (l, error, lr)\n",
    "    print 'Validation error: %.1f%%' % error_rate(\n",
    "        validation_prediction.eval(), validation_labels)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_error, confusions = error_rate(test_prediction.eval(), test_labels)\n",
    "print 'Test error: %.1f%%' % test_error\n",
    "\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(False)\n",
    "plt.xticks(numpy.arange(NUM_LABELS))\n",
    "plt.yticks(numpy.arange(NUM_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');\n",
    "\n",
    "for i, cas in enumerate(confusions):\n",
    "  for j, count in enumerate(cas):\n",
    "    if count > 0:\n",
    "      xoff = .07 * len(str(count))\n",
    "      plt.text(j-xoff, i+.2, int(count), fontsize=9, color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.xticks(numpy.arange(NUM_LABELS))\n",
    "plt.hist(numpy.argmax(test_labels, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
