{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The AWS/GPU situation is very annoying.  \n",
    "\n",
    "#I just used this pre-built AMI:\n",
    "##    https://gist.github.com/erikbern/78ba519b97b440e10640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--batch_size BATCH_SIZE] [--num_batches NUM_BATCHES]\n",
      "__main__.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-64f89975-b1c6-4e61-86ac-9eb0feaeafe5.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128,\n",
    "                            \"\"\"Batch size.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_batches', 100,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "\n",
    "parameters = []\n",
    "\n",
    "conv_counter = 1\n",
    "pool_counter = 1\n",
    "affine_counter = 1\n",
    "\n",
    "def _conv(inpOp, nIn, nOut, kH, kW, dH, dW, padType):\n",
    "    global conv_counter\n",
    "    global parameters\n",
    "    name = 'conv' + str(conv_counter)\n",
    "    conv_counter += 1\n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([kH, kW, nIn, nOut],\n",
    "                                                 dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(inpOp, kernel, [1, dH, dW, 1], padding=padType)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[nOut], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        bias = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
    "        conv1 = tf.nn.relu(bias, name=scope)\n",
    "        parameters += [kernel, biases]\n",
    "        return conv1\n",
    "\n",
    "def _affine(inpOp, nIn, nOut):\n",
    "    global affine_counter\n",
    "    global parameters\n",
    "    name = 'affine' + str(affine_counter)\n",
    "    affine_counter += 1\n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([nIn, nOut],\n",
    "                                                 dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[nOut], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        affine1 = tf.nn.relu_layer(inpOp, kernel, biases, name=name)\n",
    "        parameters += [kernel, biases]\n",
    "        return affine1\n",
    "\n",
    "def _mpool(inpOp, kH, kW, dH, dW, padding):\n",
    "    global pool_counter\n",
    "    global parameters\n",
    "    name = 'pool' + str(pool_counter)\n",
    "    pool_counter += 1\n",
    "    return tf.nn.max_pool(inpOp,\n",
    "                          ksize=[1, kH, kW, 1],\n",
    "                          strides=[1, dH, dW, 1],\n",
    "                          padding=padding,\n",
    "                          name=name)\n",
    "\n",
    "def _apool(inpOp, kH, kW, dH, dW, padding):\n",
    "    global pool_counter\n",
    "    global parameters\n",
    "    name = 'pool' + str(pool_counter)\n",
    "    pool_counter += 1\n",
    "    return tf.nn.avg_pool(inpOp,\n",
    "                          ksize=[1, kH, kW, 1],\n",
    "                          strides=[1, dH, dW, 1],\n",
    "                          padding=padding,\n",
    "                          name=name)\n",
    "\n",
    "def _inception(inp, inSize, o1s, o2s1, o2s2, o3s1, o3s2, o4s1, o4s2):\n",
    "    conv1 = _conv(inp, inSize, o1s, 1, 1, 1, 1, 'SAME')\n",
    "\n",
    "    conv3_ = _conv(inp, inSize, o2s1, 1, 1, 1, 1, 'SAME')\n",
    "    conv3 = _conv(conv3_, o2s1, o2s2, 3, 3, 1, 1, 'SAME')\n",
    "\n",
    "    conv5_ = _conv(inp, inSize, o3s1, 1, 1, 1, 1, 'SAME')\n",
    "    conv5 = _conv(conv5_, o3s1, o3s2, 5, 5, 1, 1, 'SAME')\n",
    "\n",
    "    pool_ = _mpool(inp, o4s1, o4s1, 1, 1, 'SAME')\n",
    "    pool = _conv(pool_, inSize, o4s2, 1, 1, 1, 1, 'SAME')\n",
    "\n",
    "    incept = array_ops.concat(3, [conv1, conv3, conv5, pool])\n",
    "    return incept\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "    batch_size = tf.size(labels)\n",
    "    labels = tf.expand_dims(labels, 1)\n",
    "    indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "    concated = tf.concat(1, [indices, labels])\n",
    "    onehot_labels = tf.sparse_to_dense(\n",
    "        concated, tf.pack([batch_size, 1000]), 1.0, 0.0)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,\n",
    "                                                            onehot_labels,\n",
    "                                                            name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    return loss\n",
    "\n",
    "def inference(images):\n",
    "    conv1 = _conv (images, 3, 64, 7, 7, 2, 2, 'SAME')\n",
    "    pool1 = _mpool(conv1,  3, 3, 2, 2, 'SAME')\n",
    "    conv2 = _conv (pool1,  64, 64, 1, 1, 1, 1, 'SAME')\n",
    "    conv3 = _conv (conv2,  64, 192, 3, 3, 1, 1, 'SAME')\n",
    "    pool3 = _mpool(conv3,  3, 3, 2, 2, 'SAME')\n",
    "\n",
    "    incept3a = _inception(pool3,    192, 64, 96, 128, 16, 32, 3, 32)\n",
    "    incept3b = _inception(incept3a, 256, 128, 128, 192, 32, 96, 3, 64)\n",
    "    pool4 = _mpool(incept3b,  3, 3, 2, 2, 'SAME')\n",
    "    incept4a = _inception(pool4,    480, 192,  96, 208, 16, 48, 3, 64)\n",
    "    incept4b = _inception(incept4a, 512, 160, 112, 224, 24, 64, 3, 64)\n",
    "    incept4c = _inception(incept4b, 512, 128, 128, 256, 24, 64, 3, 64)\n",
    "    incept4d = _inception(incept4c, 512, 112, 144, 288, 32, 64, 3, 64)\n",
    "    incept4e = _inception(incept4d, 528, 256, 160, 320, 32, 128, 3, 128)\n",
    "    pool5 = _mpool(incept4e,  3, 3, 2, 2, 'SAME')\n",
    "    incept5a = _inception(pool5,    832, 256, 160, 320, 32, 128, 3, 128)\n",
    "    incept5b = _inception(incept5a, 832, 384, 192, 384, 48, 128, 3, 128)\n",
    "    pool6 = _apool(incept5b,  7, 7, 1, 1, 'VALID')\n",
    "\n",
    "    resh1 = tf.reshape(pool6, [-1, 1024])\n",
    "    affn1 = _affine(resh1, 1024, 1000)\n",
    "\n",
    "    return affn1\n",
    "\n",
    "\n",
    "def time_tensorflow_run(session, target, info_string):\n",
    "  num_steps_burn_in = 10\n",
    "  total_duration = 0.0\n",
    "  total_duration_squared = 0.0\n",
    "  if not isinstance(target, list):\n",
    "    target = [target]\n",
    "  for i in xrange(FLAGS.num_batches + num_steps_burn_in):\n",
    "    start_time = time.time()\n",
    "    _ = session.run(tf.group(*target))\n",
    "    duration = time.time() - start_time\n",
    "    if i > num_steps_burn_in:\n",
    "      if not i % 10:\n",
    "        print ('%s: step %d, duration = %.3f' %\n",
    "               (datetime.now(), i - num_steps_burn_in, duration))\n",
    "      total_duration += duration\n",
    "      total_duration_squared += duration * duration\n",
    "  mn = total_duration / FLAGS.num_batches\n",
    "  vr = total_duration_squared / FLAGS.num_batches - mn * mn\n",
    "  sd = math.sqrt(vr)\n",
    "  print ('%s: %s across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "         (datetime.now(), info_string, FLAGS.num_batches, mn, sd))\n",
    "\n",
    "def run_benchmark():\n",
    "  global parameters\n",
    "  with tf.Graph().as_default():\n",
    "    # Generate some dummy images.\n",
    "    image_size = 224\n",
    "    images = tf.Variable(tf.random_normal([FLAGS.batch_size,\n",
    "                                           image_size,\n",
    "                                           image_size, 3],\n",
    "                                          dtype=tf.float32,\n",
    "                                          stddev=1e-1))\n",
    "\n",
    "    labels = tf.Variable(tf.ones([FLAGS.batch_size],\n",
    "                                 dtype=tf.int32))\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    last_layer = inference(images)\n",
    "\n",
    "    # Build an initialization operation.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Start running operations on the Graph.\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(init)\n",
    "\n",
    "    # Run the forward benchmark.\n",
    "    time_tensorflow_run(sess, last_layer, \"Forward\")\n",
    "\n",
    "    # Add a simple objective so we can calculate the backward pass.\n",
    "    objective = loss(last_layer, labels)\n",
    "    # Compute the gradient with respect to all the parameters.\n",
    "    grad = tf.gradients(objective, parameters)\n",
    "    # Run the backward benchmark.\n",
    "    time_tensorflow_run(sess, grad, \"Forward-backward\")\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  run_benchmark()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
