{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:  Convert to Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Connection to Kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-76c4a839a884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#import org.apache.spark.sql.functions.json_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstreamingInputDF\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<server:ip>\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subscribe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"topic1\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"startingOffsets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"latest\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minPartitions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"10\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"failOnDataLoss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.get_json_object\n",
    "import org.apache.spark.sql.functions.json_tuple\n",
    " \n",
    "streamingInputDF = \\\n",
    "  spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"<server:ip>\") \\\n",
    "    .option(\"subscribe\", \"topic1\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"minPartitions\", \"10\") \\\n",
    "    .option(\"failOnDataLoss\", \"true\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### streamingInputDF.printSchema\n",
    "```\n",
    "root \n",
    "   |-- key: binary (nullable = true) \n",
    "\n",
    "   |-- value: binary (nullable = true) \n",
    "\n",
    "   |-- topic: string (nullable = true) \n",
    "\n",
    "   |-- partition: integer (nullable = true) \n",
    "\n",
    "   |-- offset: long (nullable = true) \n",
    "\n",
    "   |-- timestamp: timestamp (nullable = true) \n",
    "\n",
    "   |-- timestampType: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Message\n",
    "```\n",
    "{\n",
    "\"city\": \"\", \n",
    "\"country\": \"United States\", \n",
    "\"countryCode\": \"US\", \n",
    "\"isp\": \"\", \n",
    "\"lat\": 0.00, \"lon\": 0.00, \n",
    "\"query\": \"\", \n",
    "\"region\": \"CA\", \n",
    "\"regionName\": \"California\", \n",
    "\"status\": \"success\", \n",
    "\"hittime\": \"2017-02-08T17:37:55-05:00\", \n",
    "\"zip\": \"38917\" \n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy, Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-808b6c4342b9>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-808b6c4342b9>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    var streamingSelectDF =\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "var streamingSelectDF = \n",
    "  streamingInputDF\n",
    "   .select(get_json_object(($\"value\").cast(\"string\"), \"$.zip\").alias(\"zip\"))\n",
    "    .groupBy($\"zip\") \n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "var streamingSelectDF = \n",
    "  streamingInputDF\n",
    "   .select(get_json_object(($\"value\").cast(\"string\"), \"$.zip\").alias(\"zip\"), get_json_object(($\"value\").cast(\"string\"), \"$.hittime\").alias(\"hittime\"))\n",
    "   .groupBy($\"zip\", window($\"hittime\".cast(\"timestamp\"), \"10 minute\", \"5 minute\", \"2 minute\"))\n",
    "   .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.streaming.ProcessingTime\n",
    "\n",
    "val query =\n",
    "  streamingSelectDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        \n",
    "    .queryName(\"isphits\")     \n",
    "    .outputMode(\"complete\") \n",
    "    .trigger(ProcessingTime(\"25 seconds\"))\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Console Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-9ca019d0f54d>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-9ca019d0f54d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    val query =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.ProcessingTime\n",
    "\n",
    "val query =\n",
    "  streamingSelectDF\n",
    "    .writeStream\n",
    "    .format(\"console\")        \n",
    "    .outputMode(\"complete\") \n",
    "    .trigger(ProcessingTime(\"25 seconds\"))\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Output with Partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "var streamingSelectDF = \n",
    "  streamingInputDF\n",
    "   .select(get_json_object(($\"value\").cast(\"string\"), \"$.zip\").alias(\"zip\"),    get_json_object(($\"value\").cast(\"string\"), \"$.hittime\").alias(\"hittime\"), date_format(get_json_object(($\"value\").cast(\"string\"), \"$.hittime\"), \"dd.MM.yyyy\").alias(\"day\"))\n",
    "    .groupBy($\"zip\") \n",
    "    .count()\n",
    "    .as[(String, String)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql CREATE EXTERNAL TABLE  test_par\n",
    "    (hittime string)\n",
    "    PARTITIONED BY (zip string, day string)\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '/mnt/sample/test-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql ALTER TABLE test_par ADD IF NOT EXISTS\n",
    "    PARTITION (zip='38907', day='08.02.2017') LOCATION '/mnt/sample/test-data/zip=38907/day=08.02.2017'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from test_par\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JDBC Sink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.sql._\n",
    "\n",
    "class  JDBCSink(url:String, user:String, pwd:String) extends ForeachWriter[(String, String)] {\n",
    "      val driver = \"com.mysql.jdbc.Driver\"\n",
    "      var connection:Connection = _\n",
    "      var statement:Statement = _\n",
    "      \n",
    "    def open(partitionId: Long,version: Long): Boolean = {\n",
    "        Class.forName(driver)\n",
    "        connection = DriverManager.getConnection(url, user, pwd)\n",
    "        statement = connection.createStatement\n",
    "        true\n",
    "      }\n",
    "\n",
    "      def process(value: (String, String)): Unit = {\n",
    "        statement.executeUpdate(\"INSERT INTO zip_test \" + \n",
    "                \"VALUES (\" + value._1 + \",\" + value._2 + \")\")\n",
    "      }\n",
    "\n",
    "      def close(errorOrNull: Throwable): Unit = {\n",
    "        connection.close\n",
    "      }\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val url=\"jdbc:mysql://<mysqlserver>:3306/test\"\n",
    "val user =\"user\"\n",
    "val pwd = \"pwd\"\n",
    "\n",
    "val writer = new JDBCSink(url,user, pwd)\n",
    "val query =\n",
    "  streamingSelectDF\n",
    "    .writeStream\n",
    "    .foreach(writer)\n",
    "    .outputMode(\"update\")\n",
    "    .trigger(ProcessingTime(\"25 seconds\"))\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Sink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.Properties\n",
    "import kafkashaded.org.apache.kafka.clients.producer._\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "\n",
    "\n",
    " class  KafkaSink(topic:String, servers:String) extends ForeachWriter[(String, String)] {\n",
    "      val kafkaProperties = new Properties()\n",
    "      kafkaProperties.put(\"bootstrap.servers\", servers)\n",
    "      kafkaProperties.put(\"key.serializer\", \"kafkashaded.org.apache.kafka.common.serialization.StringSerializer\")\n",
    "      kafkaProperties.put(\"value.serializer\", \"kafkashaded.org.apache.kafka.common.serialization.StringSerializer\")\n",
    "      val results = new scala.collection.mutable.HashMap[String, String]\n",
    "      var producer: KafkaProducer[String, String] = _\n",
    "\n",
    "      def open(partitionId: Long,version: Long): Boolean = {\n",
    "        producer = new KafkaProducer(kafkaProperties)\n",
    "        true\n",
    "      }\n",
    "\n",
    "      def process(value: (String, String)): Unit = {\n",
    "          producer.send(new ProducerRecord(topic, value._1 + \":\" + value._2))\n",
    "      }\n",
    "\n",
    "      def close(errorOrNull: Throwable): Unit = {\n",
    "        producer.close()\n",
    "      }\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-57250a802922>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-57250a802922>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    val topic = \"<topic2>\"\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val topic = \"<topic2>\"\n",
    "val brokers = \"<server:ip>\"\n",
    "\n",
    "val writer = new KafkaSink(topic, brokers)\n",
    "\n",
    "val query =\n",
    "  streamingSelectDF\n",
    "    .writeStream\n",
    "    .foreach(writer)\n",
    "    .outputMode(\"update\")\n",
    "    .trigger(ProcessingTime(\"25 seconds\"))\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
