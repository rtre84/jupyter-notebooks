{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL4.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL2.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL3.png' width=\"80%\" height=\"80%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/sparkSQL1.png' width=\"80%\" height=\"80%\"></img>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Getting started:\n",
    "Create a SQL Context from the Spark Context, sc, which is predefined in every notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SQL Context queries Dataframes, not RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A data file on world banks will downloaded from GitHub after removing any previous data that may exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-03-30 16:20:21--  https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 23.235.47.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|23.235.47.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 446287 (436K) [application/octet-stream]\n",
      "Saving to: 'world_bank.json.gz'\n",
      "\n",
      "100%[======================================>] 446,287     --.-K/s   in 0.04s   \n",
      "\n",
      "2016-03-30 16:20:22 (11.7 MB/s) - 'world_bank.json.gz' saved [446287/446287]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm world_bank* -f\n",
    "!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#A Dataframe will be created using the sqlContext to read the file. Many other types are supported including text and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example1_df = sqlContext.read.json(\"world_bank.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Spark SQL has the ability to infer the schema of JSON data and understand the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- approvalfy: string (nullable = true)\n",
      " |-- board_approval_month: string (nullable = true)\n",
      " |-- boardapprovaldate: string (nullable = true)\n",
      " |-- borrower: string (nullable = true)\n",
      " |-- closingdate: string (nullable = true)\n",
      " |-- country_namecode: string (nullable = true)\n",
      " |-- countrycode: string (nullable = true)\n",
      " |-- countryname: string (nullable = true)\n",
      " |-- countryshortname: string (nullable = true)\n",
      " |-- docty: string (nullable = true)\n",
      " |-- envassesmentcategorycode: string (nullable = true)\n",
      " |-- grantamt: long (nullable = true)\n",
      " |-- ibrdcommamt: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- idacommamt: long (nullable = true)\n",
      " |-- impagency: string (nullable = true)\n",
      " |-- lendinginstr: string (nullable = true)\n",
      " |-- lendinginstrtype: string (nullable = true)\n",
      " |-- lendprojectcost: long (nullable = true)\n",
      " |-- majorsector_percent: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Name: string (nullable = true)\n",
      " |    |    |-- Percent: long (nullable = true)\n",
      " |-- mjsector_namecode: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- mjtheme: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- mjtheme_namecode: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- mjthemecode: string (nullable = true)\n",
      " |-- prodline: string (nullable = true)\n",
      " |-- prodlinetext: string (nullable = true)\n",
      " |-- productlinetype: string (nullable = true)\n",
      " |-- project_abstract: struct (nullable = true)\n",
      " |    |-- cdata: string (nullable = true)\n",
      " |-- project_name: string (nullable = true)\n",
      " |-- projectdocs: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- DocDate: string (nullable = true)\n",
      " |    |    |-- DocType: string (nullable = true)\n",
      " |    |    |-- DocTypeDesc: string (nullable = true)\n",
      " |    |    |-- DocURL: string (nullable = true)\n",
      " |    |    |-- EntityID: string (nullable = true)\n",
      " |-- projectfinancialtype: string (nullable = true)\n",
      " |-- projectstatusdisplay: string (nullable = true)\n",
      " |-- regionname: string (nullable = true)\n",
      " |-- sector: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Name: string (nullable = true)\n",
      " |-- sector1: struct (nullable = true)\n",
      " |    |-- Name: string (nullable = true)\n",
      " |    |-- Percent: long (nullable = true)\n",
      " |-- sector2: struct (nullable = true)\n",
      " |    |-- Name: string (nullable = true)\n",
      " |    |-- Percent: long (nullable = true)\n",
      " |-- sector3: struct (nullable = true)\n",
      " |    |-- Name: string (nullable = true)\n",
      " |    |-- Percent: long (nullable = true)\n",
      " |-- sector4: struct (nullable = true)\n",
      " |    |-- Name: string (nullable = true)\n",
      " |    |-- Percent: long (nullable = true)\n",
      " |-- sector_namecode: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- sectorcode: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- supplementprojectflg: string (nullable = true)\n",
      " |-- theme1: struct (nullable = true)\n",
      " |    |-- Name: string (nullable = true)\n",
      " |    |-- Percent: long (nullable = true)\n",
      " |-- theme_namecode: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- code: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- themecode: string (nullable = true)\n",
      " |-- totalamt: long (nullable = true)\n",
      " |-- totalcommamt: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print example1_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Let's take a look at the first two rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(_id=Row($oid=u'52b213b38594d8a2be17c780'), approvalfy=u'1999', board_approval_month=u'November', boardapprovaldate=u'2013-11-12T00:00:00Z', borrower=u'FEDERAL DEMOCRATIC REPUBLIC OF ETHIOPIA', closingdate=u'2018-07-07T00:00:00Z', country_namecode=u'Federal Democratic Republic of Ethiopia!$!ET', countrycode=u'ET', countryname=u'Federal Democratic Republic of Ethiopia', countryshortname=u'Ethiopia', docty=u'Project Information Document,Indigenous Peoples Plan,Project Information Document', envassesmentcategorycode=u'C', grantamt=0, ibrdcommamt=0, id=u'P129828', idacommamt=130000000, impagency=u'MINISTRY OF EDUCATION', lendinginstr=u'Investment Project Financing', lendinginstrtype=u'IN', lendprojectcost=550000000, majorsector_percent=[Row(Name=u'Education', Percent=46), Row(Name=u'Education', Percent=26), Row(Name=u'Public Administration, Law, and Justice', Percent=16), Row(Name=u'Education', Percent=12)], mjsector_namecode=[Row(code=u'EX', name=u'Education'), Row(code=u'EX', name=u'Education'), Row(code=u'BX', name=u'Public Administration, Law, and Justice'), Row(code=u'EX', name=u'Education')], mjtheme=[u'Human development'], mjtheme_namecode=[Row(code=u'8', name=u'Human development'), Row(code=u'11', name=u'')], mjthemecode=u'8,11', prodline=u'PE', prodlinetext=u'IBRD/IDA', productlinetype=u'L', project_abstract=Row(cdata=u'The development objective of the Second Phase of General Education Quality Improvement Project for Ethiopia is to improve learning conditions in primary and secondary schools and strengthen institutions at different levels of educational administration. The project has six components. The first component is curriculum, textbooks, assessment, examinations, and inspection. This component will support improvement of learning conditions in grades KG-12 by providing increased access to teaching and learning materials and through improvements to the curriculum by assessing the strengths and weaknesses of the current curriculum. This component has following four sub-components: (i) curriculum reform and implementation; (ii) teaching and learning materials; (iii) assessment and examinations; and (iv) inspection. The second component is teacher development program (TDP). This component will support improvements in learning conditions in both primary and secondary schools by advancing the quality of teaching in general education through: (a) enhancing the training of pre-service teachers in teacher education institutions; and (b) improving the quality of in-service teacher training. This component has following three sub-components: (i) pre-service teacher training; (ii) in-service teacher training; and (iii) licensing and relicensing of teachers and school leaders. The third component is school improvement plan. This component will support the strengthening of school planning in order to improve learning outcomes, and to partly fund the school improvement plans through school grants. It has following two sub-components: (i) school improvement plan; and (ii) school grants. The fourth component is management and capacity building, including education management information systems (EMIS). This component will support management and capacity building aspect of the project. This component has following three sub-components: (i) capacity building for education planning and management; (ii) capacity building for school planning and management; and (iii) EMIS. The fifth component is improving the quality of learning and teaching in secondary schools and universities through the use of information and communications technology (ICT). It has following five sub-components: (i) national policy and institution for ICT in general education; (ii) national ICT infrastructure improvement plan for general education; (iii) develop an integrated monitoring, evaluation, and learning system specifically for the ICT component; (iv) teacher professional development in the use of ICT; and (v) provision of limited number of e-Braille display readers with the possibility to scale up to all secondary education schools based on the successful implementation and usage of the readers. The sixth component is program coordination, monitoring and evaluation, and communication. It will support institutional strengthening by developing capacities in all aspects of program coordination, monitoring and evaluation; a new sub-component on communications will support information sharing for better management and accountability. It has following three sub-components: (i) program coordination; (ii) monitoring and evaluation (M and E); and (iii) communication.'), project_name=u'Ethiopia General Education Quality Improvement Project II', projectdocs=[Row(DocDate=u'28-AUG-2013', DocType=u'PID', DocTypeDesc=u'Project Information Document (PID),  Vol.', DocURL=u'http://www-wds.worldbank.org/servlet/WDSServlet?pcont=details&eid=090224b081e545fb_1_0', EntityID=u'090224b081e545fb_1_0'), Row(DocDate=u'01-JUL-2013', DocType=u'IP', DocTypeDesc=u'Indigenous Peoples Plan (IP),  Vol.1 of 1', DocURL=u'http://www-wds.worldbank.org/servlet/WDSServlet?pcont=details&eid=000442464_20130920111729', EntityID=u'000442464_20130920111729'), Row(DocDate=u'22-NOV-2012', DocType=u'PID', DocTypeDesc=u'Project Information Document (PID),  Vol.', DocURL=u'http://www-wds.worldbank.org/servlet/WDSServlet?pcont=details&eid=090224b0817b19e2_1_0', EntityID=u'090224b0817b19e2_1_0')], projectfinancialtype=u'IDA', projectstatusdisplay=u'Active', regionname=u'Africa', sector=[Row(Name=u'Primary education'), Row(Name=u'Secondary education'), Row(Name=u'Public administration- Other social services'), Row(Name=u'Tertiary education')], sector1=Row(Name=u'Primary education', Percent=46), sector2=Row(Name=u'Secondary education', Percent=26), sector3=Row(Name=u'Public administration- Other social services', Percent=16), sector4=Row(Name=u'Tertiary education', Percent=12), sector_namecode=[Row(code=u'EP', name=u'Primary education'), Row(code=u'ES', name=u'Secondary education'), Row(code=u'BS', name=u'Public administration- Other social services'), Row(code=u'ET', name=u'Tertiary education')], sectorcode=u'ET,BS,ES,EP', source=u'IBRD', status=u'Active', supplementprojectflg=u'N', theme1=Row(Name=u'Education for all', Percent=100), theme_namecode=[Row(code=u'65', name=u'Education for all')], themecode=u'65', totalamt=130000000, totalcommamt=130000000, url=u'http://www.worldbank.org/projects/P129828/ethiopia-general-education-quality-improvement-project-ii?lang=en')\n",
      "********************\n",
      "Row(_id=Row($oid=u'52b213b38594d8a2be17c781'), approvalfy=u'2015', board_approval_month=u'November', boardapprovaldate=u'2013-11-04T00:00:00Z', borrower=u'GOVERNMENT OF TUNISIA', closingdate=None, country_namecode=u'Republic of Tunisia!$!TN', countrycode=u'TN', countryname=u'Republic of Tunisia', countryshortname=u'Tunisia', docty=u'Project Information Document,Integrated Safeguards Data Sheet,Integrated Safeguards Data Sheet,Project Information Document,Integrated Safeguards Data Sheet,Project Information Document', envassesmentcategorycode=u'C', grantamt=4700000, ibrdcommamt=0, id=u'P144674', idacommamt=0, impagency=u'MINISTRY OF FINANCE', lendinginstr=u'Specific Investment Loan', lendinginstrtype=u'IN', lendprojectcost=5700000, majorsector_percent=[Row(Name=u'Public Administration, Law, and Justice', Percent=70), Row(Name=u'Public Administration, Law, and Justice', Percent=30)], mjsector_namecode=[Row(code=u'BX', name=u'Public Administration, Law, and Justice'), Row(code=u'BX', name=u'Public Administration, Law, and Justice')], mjtheme=[u'Economic management', u'Social protection and risk management'], mjtheme_namecode=[Row(code=u'1', name=u'Economic management'), Row(code=u'6', name=u'Social protection and risk management')], mjthemecode=u'1,6', prodline=u'RE', prodlinetext=u'Recipient Executed Activities', productlinetype=u'L', project_abstract=None, project_name=u'TN: DTF Social Protection Reforms Support', projectdocs=[Row(DocDate=u'29-MAR-2013', DocType=u'PID', DocTypeDesc=u'Project Information Document (PID),  Vol.1 of 1', DocURL=u'http://www-wds.worldbank.org/servlet/WDSServlet?pcont=details&eid=000333037_20131024115616', EntityID=u'000333037_20131024115616'), Row(DocDate=u'29-MAR-2013', DocType=u'ISDS', DocTypeDesc=u'Integrated Safeguards Data Sheet (ISDS),  Vol.1 of 1', DocURL=u'http://www-wds.worldbank.org/servlet/WDSServlet?pcont=details&eid=000356161_20131024151611', EntityID=u'000356161_20131024151611'), Row(DocDate=u'29-MAR-2013', DocType=u'ISDS', DocTypeDesc=u'Integrated Safeguards Data Sheet (ISDS),  Vol.1 of 1', DocURL=u'http://www-wds.worldbank.org/servlet/WDSServlet?pcont=details&eid=000442464_20131031112136', EntityID=u'000442464_20131031112136'), Row(DocDate=u'29-MAR-2013', DocType=u'PID', DocTypeDesc=u'Project Information Document (PID),  Vol.1 of 1', DocURL=u'http://www-wds.worldbank.org/servlet/WDSServlet?pcont=details&eid=000333037_20131031105716', EntityID=u'000333037_20131031105716'), Row(DocDate=u'16-JAN-2013', DocType=u'ISDS', DocTypeDesc=u'Integrated Safeguards Data Sheet (ISDS),  Vol.1 of 1', DocURL=u'http://www-wds.worldbank.org/servlet/WDSServlet?pcont=details&eid=000356161_20130305113209', EntityID=u'000356161_20130305113209'), Row(DocDate=u'16-JAN-2013', DocType=u'PID', DocTypeDesc=u'Project Information Document (PID),  Vol.1 of 1', DocURL=u'http://www-wds.worldbank.org/servlet/WDSServlet?pcont=details&eid=000356161_20130305113716', EntityID=u'000356161_20130305113716')], projectfinancialtype=u'OTHER', projectstatusdisplay=u'Active', regionname=u'Middle East and North Africa', sector=[Row(Name=u'Public administration- Other social services'), Row(Name=u'General public administration sector')], sector1=Row(Name=u'Public administration- Other social services', Percent=70), sector2=Row(Name=u'General public administration sector', Percent=30), sector3=None, sector4=None, sector_namecode=[Row(code=u'BS', name=u'Public administration- Other social services'), Row(code=u'BZ', name=u'General public administration sector')], sectorcode=u'BZ,BS', source=u'IBRD', status=u'Active', supplementprojectflg=u'N', theme1=Row(Name=u'Other economic management', Percent=30), theme_namecode=[Row(code=u'24', name=u'Other economic management'), Row(code=u'54', name=u'Social safety nets')], themecode=u'54,24', totalamt=0, totalcommamt=4700000, url=u'http://www.worldbank.org/projects/P144674?lang=en')\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "for row in example1_df.take(2):\n",
    "    print row\n",
    "    print \"*\" * 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now let's register a table which is a pointer to the Dataframe and allows data access via Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simply use the Dataframe Object to create the table:\n",
    "example1_df.registerTempTable(\"world_bank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "********************\n",
      "DataFrame[_id: struct<$oid:string>, approvalfy: string, board_approval_month: string, boardapprovaldate: string, borrower: string, closingdate: string, country_namecode: string, countrycode: string, countryname: string, countryshortname: string, docty: string, envassesmentcategorycode: string, grantamt: bigint, ibrdcommamt: bigint, id: string, idacommamt: bigint, impagency: string, lendinginstr: string, lendinginstrtype: string, lendprojectcost: bigint, majorsector_percent: array<struct<Name:string,Percent:bigint>>, mjsector_namecode: array<struct<code:string,name:string>>, mjtheme: array<string>, mjtheme_namecode: array<struct<code:string,name:string>>, mjthemecode: string, prodline: string, prodlinetext: string, productlinetype: string, project_abstract: struct<cdata:string>, project_name: string, projectdocs: array<struct<DocDate:string,DocType:string,DocTypeDesc:string,DocURL:string,EntityID:string>>, projectfinancialtype: string, projectstatusdisplay: string, regionname: string, sector: array<struct<Name:string>>, sector1: struct<Name:string,Percent:bigint>, sector2: struct<Name:string,Percent:bigint>, sector3: struct<Name:string,Percent:bigint>, sector4: struct<Name:string,Percent:bigint>, sector_namecode: array<struct<code:string,name:string>>, sectorcode: string, source: string, status: string, supplementprojectflg: string, theme1: struct<Name:string,Percent:bigint>, theme_namecode: array<struct<code:string,name:string>>, themecode: string, totalamt: bigint, totalcommamt: bigint, url: string]\n"
     ]
    }
   ],
   "source": [
    "#now that the table is registered we can execute sql commands \n",
    "#NOTE that the returned object is another Dataframe:\n",
    "\n",
    "temp_df =  sqlContext.sql(\"select * from world_bank limit 2\")\n",
    "\n",
    "print type(temp_df)\n",
    "print \"*\" * 20\n",
    "print temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>borrower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P129828</td>\n",
       "      <td>FEDERAL DEMOCRATIC REPUBLIC OF ETHIOPIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P144674</td>\n",
       "      <td>GOVERNMENT OF TUNISIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                 borrower\n",
       "0  P129828  FEDERAL DEMOCRATIC REPUBLIC OF ETHIOPIA\n",
       "1  P144674                    GOVERNMENT OF TUNISIA"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one nice feature of the notebooks and python is that we can show it in a table via Pandas\n",
    "sqlContext.sql(\"select id, borrower from world_bank limit 2\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regionname</th>\n",
       "      <th>project_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Africa</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East Asia and Pacific</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europe and Central Asia</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Asia</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Middle East and North Africa</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     regionname  project_count\n",
       "0                        Africa            152\n",
       "1         East Asia and Pacific            100\n",
       "2       Europe and Central Asia             74\n",
       "3                    South Asia             65\n",
       "4  Middle East and North Africa             54\n",
       "5   Latin America and Caribbean             53\n",
       "6                         Other              2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is a simple group by example:\n",
    "\n",
    "query = \"\"\"\n",
    "select\n",
    "    regionname ,\n",
    "    count(*) as project_count\n",
    "from world_bank\n",
    "group by regionname \n",
    "order by count(*) desc\n",
    "\"\"\"\n",
    "\n",
    "sqlContext.sql(query).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regionname</th>\n",
       "      <th>project_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Africa</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East Asia and Pacific</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              regionname  project_count\n",
       "0                 Africa            152\n",
       "1  East Asia and Pacific            100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subselect works as well:\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "select * from\n",
    "    (select\n",
    "        regionname ,\n",
    "        count(*) as project_count\n",
    "    from world_bank\n",
    "    group by regionname \n",
    "    order by count(*) desc) table_alias\n",
    "limit 2\n",
    "\"\"\"\n",
    "\n",
    "sqlContext.sql(query).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Simple Example of Adding a Schema (headers) to an RDD and using it as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below a simple RDD is created with Random Data in two columns and an ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 3], [2, 1, 3], [3, 3, 1], [4, 0, 2], [5, 6, 4]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#first let's create a simple RDD\n",
    "\n",
    "#create a Python list of lists for our example\n",
    "data_e2 = []\n",
    "for x in range(1,6):\n",
    "    random_int = int(random.random() * 10)\n",
    "    data_e2.append([x, random_int, random_int^2])\n",
    "\n",
    "#create the RDD with the random list of lists\n",
    "rdd_example2 = sc.parallelize(data_e2)\n",
    "print rdd_example2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(ID=u'1', VAL1=u'1', VAL2=u'3'), Row(ID=u'2', VAL1=u'1', VAL2=u'3'), Row(ID=u'3', VAL1=u'3', VAL2=u'1'), Row(ID=u'4', VAL1=u'0', VAL2=u'2'), Row(ID=u'5', VAL1=u'6', VAL2=u'4')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#now we can assign some header information\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"ID VAL1 VAL2\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaExample = sqlContext.createDataFrame(rdd_example2, schema)\n",
    "\n",
    "# Register the DataFrame as a table.\n",
    "schemaExample.registerTempTable(\"example2\")\n",
    "\n",
    "# Pull the data\n",
    "print schemaExample.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 3\n",
      "2 1 3\n"
     ]
    }
   ],
   "source": [
    "#In Dataframes we can reference the columns names for example:\n",
    "\n",
    "for row in schemaExample.take(2):\n",
    "    print row.ID, row.VAL1, row.VAL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>VAL1</th>\n",
       "      <th>VAL2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID VAL1 VAL2\n",
       "0  1    1    3\n",
       "1  2    1    3\n",
       "2  3    3    1\n",
       "3  4    0    2\n",
       "4  5    6    4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Again a simple sql example:\n",
    "\n",
    "sqlContext.sql(\"select * from example2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Another Example of creating a Dataframe from an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "[[1, 1, 3], [2, 1, 3], [3, 3, 1], [4, 0, 2], [5, 6, 4]]\n"
     ]
    }
   ],
   "source": [
    "#Remember this RDD:\n",
    "print type(rdd_example2)\n",
    "print rdd_example2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, val1=1, val2=3), Row(id=2, val1=1, val2=3), Row(id=3, val1=3, val2=1), Row(id=4, val1=0, val2=2), Row(id=5, val1=6, val2=4)]\n"
     ]
    }
   ],
   "source": [
    "#we can use Row to specify the name of the columns with a Map, then use that to create the Dataframe\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd_example3 = rdd_example2.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2]))\n",
    "\n",
    "print rdd_example3.collect()\n",
    "                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#now we can convert rdd_example3 to a Dataframe\n",
    "\n",
    "df_example3 = rdd_example3.toDF()\n",
    "df_example3.registerTempTable(\"df_example3\")\n",
    "\n",
    "print type(df_example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  val1  val2\n",
       "0   1     1     3\n",
       "1   2     1     3\n",
       "2   3     3     1\n",
       "3   4     0     2\n",
       "4   5     6     4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now a simple SQL statement\n",
    "sqlContext.sql(\"select * from df_example3\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Joins are supported, here is a simple example with our two new tables\n",
    "We can join example2 and example3 on ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve 'e2.id' given input columns VAL1, ID, val2, val1, id, VAL2;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1f4835386318>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \"\"\"\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/root/spark-1.6.0-bin-fluxcapacitor/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \"\"\"\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/spark-1.6.0-bin-fluxcapacitor/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/spark-1.6.0-bin-fluxcapacitor/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     49\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: u\"cannot resolve 'e2.id' given input columns VAL1, ID, val2, val1, id, VAL2;\""
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select\n",
    "    *\n",
    "from\n",
    "    example2 e2\n",
    "inner join df_example3 e3 on\n",
    "    e2.id = e3.id\n",
    "\"\"\"\n",
    "\n",
    "print sqlContext.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Alternatively you can join within Python as well\n",
    "\n",
    "df_example4 = df_example3.join(schemaExample, schemaExample[\"id\"] == df_example3[\"ID\"] )\n",
    "\n",
    "for row in df_example4.take(5):\n",
    "    print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#One of the more powerful features is the ability to create Functions and Use them in SQL Here is a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first we create a Python function:\n",
    "\n",
    "def simple_function(v):\n",
    "    return int(v * 10)\n",
    "\n",
    "#test the function\n",
    "print simple_function(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we can register the function for use in SQL\n",
    "sqlContext.registerFunction(\"simple_function\", simple_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we can apply the filter in a SQL Statement\n",
    "query = \"\"\"\n",
    "select\n",
    "    ID,\n",
    "    VAL1,\n",
    "    VAL2,\n",
    "    simple_function(VAL1) as s_VAL1,\n",
    "    simple_function(VAL2) as s_VAL1\n",
    "from\n",
    " example2\n",
    "\"\"\"\n",
    "sqlContext.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#note that the VAL1 and VAL2 look like strings, we can cast them as well\n",
    "query = \"\"\"\n",
    "select\n",
    "    ID,\n",
    "    VAL1,\n",
    "    VAL2,\n",
    "    simple_function(cast(VAL1 as int)) as s_VAL1,\n",
    "    simple_function(cast(VAL2 as int)) as s_VAL1\n",
    "from\n",
    " example2\n",
    "\"\"\"\n",
    "sqlContext.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Pandas Example\n",
    "Pandas is a common abstraction for working with data in Python.\n",
    "\n",
    "We can turn Pandas Dataframes into Spark Dataframes, the advantage of this \n",
    "could be scale or allowing us to run SQL statements agains the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pandas library\n",
    "import pandas as pd\n",
    "print pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's grab some UFO data to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm SIGHTINGS.csv -f\n",
    "!wget https://www.quandl.com/api/v3/datasets/NUFORC/SIGHTINGS.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using the CSV file from earlier, we can create a Pandas Dataframe:\n",
    "pandas_df = pd.read_csv(\"SIGHTINGS.csv\")\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now convert to Spark Dataframe\n",
    "spark_df = sqlContext.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#explore the first two rows:\n",
    "\n",
    "for row in spark_df.take(2):\n",
    "    print row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#register the Spark Dataframe as a table\n",
    "spark_df.registerTempTable(\"ufo_sightings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now a SQL statement\n",
    "print sqlContext.sql(\"select * from ufo_sightings limit 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Visualizing the Data\n",
    "Here are some simple ways to create charts using Pandas output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to display in the notebook we need to tell matplotlib to render inline\n",
    "at this point import the supporting libraries as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can call a function \"plot\" to create the charts.\n",
    "Since most charts are created from aggregates the record\n",
    "set should be small enough to store in Pandas\n",
    "\n",
    "We can take our UFO data from before and create a \n",
    "Pandas Dataframe from the Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufos_df = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot we call the \"plot\" method and specify the type, x and y axis columns\n",
    "and optionally the size of the chart.\n",
    "\n",
    "Many more details can be found here:\n",
    "http://pandas.pydata.org/pandas-docs/stable/visualization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufos_df.plot(kind='bar', x='Reports', y='Count', figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look good, there are too many observations, let's check how many:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sqlContext.sql(\"select count(*) from ufo_sightings limit 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ideally we could just group by year, there are many ways we could solve that:</h2>\n",
    "\n",
    "1) parse the Reports column in SQL and output the year, then group on it\n",
    "2) create a simple Python function to parse the year and call it via sql\n",
    "3) as shown below: use map against the Dataframe and append a new column with \"year\"\n",
    "\n",
    "Tge example below takes the existing data for each row and appends a new column \"year\" \n",
    "by taking the first for characters from the Reports column\n",
    "\n",
    "Reports looks like this for example:\n",
    "2016-01-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufos_df = spark_df.map(lambda x: Row(**dict(x.asDict(), year=int(x.Reports[0:4]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check to verify we get the expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ufos_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the new Dataframe as a table \"ufo_withyear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufos_df.registerTempTable(\"ufo_withyear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can group by year, order by year and filter to the last 66 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select \n",
    "    sum(count) as count, \n",
    "    year \n",
    "from ufo_withyear\n",
    "where year > 1950\n",
    "group by year\n",
    "order by year\n",
    "\"\"\"\n",
    "pandas_ufos_withyears = sqlContext.sql(query).toPandas()\n",
    "pandas_ufos_withyears.plot(kind='bar', x='year', y='count', figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
