{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.144344\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PythonPi\").getOrCreate()\n",
    "\n",
    "#partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "partitions = 10\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sourceBytes = '                                                                                         \\n\\\n",
    "import sys                                                                                              \\n\\\n",
    "from random import random                                                                               \\n\\\n",
    "from operator import add                                                                                \\n\\\n",
    "                                                                                                        \\n\\\n",
    "from pyspark.sql import SparkSession                                                                    \\n\\\n",
    "                                                                                                        \\n\\\n",
    "spark = SparkSession.builder.appName(\"PythonPi\").getOrCreate()                                          \\n\\\n",
    "                                                                                                        \\n\\\n",
    "partitions = 10                                                                                         \\n\\\n",
    "n = 100000 * partitions                                                                                 \\n\\\n",
    "                                                                                                        \\n\\\n",
    "def f(_):                                                                                               \\n\\\n",
    "    x = random() * 2 - 1                                                                                \\n\\\n",
    "    y = random() * 2 - 1                                                                                \\n\\\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0                                                             \\n\\\n",
    "                                                                                                        \\n\\\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)                  \\n\\\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))                                                           \\n\\\n",
    "                                                                                                        \\n\\\n",
    "spark.stop()                                                                                            \\n\\\n",
    "'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                         \n",
      "import sys                                                                                              \n",
      "from random import random                                                                               \n",
      "from operator import add                                                                                \n",
      "                                                                                                        \n",
      "from pyspark.sql import SparkSession                                                                    \n",
      "                                                                                                        \n",
      "spark = SparkSession.builder.appName(\"PythonPi\").getOrCreate()                                          \n",
      "                                                                                                        \n",
      "partitions = 10                                                                                         \n",
      "n = 100000 * partitions                                                                                 \n",
      "                                                                                                        \n",
      "def f(_):                                                                                               \n",
      "    x = random() * 2 - 1                                                                                \n",
      "    y = random() * 2 - 1                                                                                \n",
      "    return 1 if x ** 2 + y ** 2 <= 1 else 0                                                             \n",
      "                                                                                                        \n",
      "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)                  \n",
      "print(\"Pi is roughly %f\" % (4.0 * count / n))                                                           \n",
      "                                                                                                        \n",
      "spark.stop()                                                                                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sourceBytes.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This can be removed once the Docker file for jupyterhb is intact\n",
    "!cd ~ && rm scripts && ln -s /root/pipeline/jupyterhub.ml/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/scripts/pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                         \r\n",
      "import sys                                                                                              \r\n",
      "from random import random                                                                               \r\n",
      "from operator import add                                                                                \r\n",
      "                                                                                                        \r\n",
      "from pyspark.sql import SparkSession                                                                    \r\n",
      "                                                                                                        \r\n",
      "spark = SparkSession.builder.appName(\"PythonPi\").getOrCreate()                                          \r\n",
      "                                                                                                        \r\n",
      "partitions = 10                                                                                         \r\n",
      "n = 100000 * partitions                                                                                 \r\n",
      "                                                                                                        \r\n",
      "def f(_):                                                                                               \r\n",
      "    x = random() * 2 - 1                                                                                \r\n",
      "    y = random() * 2 - 1                                                                                \r\n",
      "    return 1 if x ** 2 + y ** 2 <= 1 else 0                                                             \r\n",
      "                                                                                                        \r\n",
      "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)                  \r\n",
      "print(\"Pi is roughly %f\" % (4.0 * count / n))                                                           \r\n",
      "                                                                                                        \r\n",
      "spark.stop()                                                                                            \r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/scripts/pi/\n",
    "\n",
    "with open('/root/scripts/pi/pi.py', 'wb') as f:\n",
    "  f.write(sourceBytes)\n",
    "\n",
    "!cat ~/scripts/pi/pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is up-to-date with 'origin/master'.\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n",
      "\r\n",
      "\t\u001b[31mmodified:   ../../Conferences/StartupML/Jan-20-2017/SparkMLTensorflowAI-HybridCloud-ContinuousDeployment-Azure.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../Scikit-Learn/DecisionTreeIris.pmml\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../Scikit-Learn/Scikit-Learn-PMML-Deploy.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   PySpark - Calculate Pi.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../TensorFlow/DeepDream/deepdream.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../TensorFlow/GPU/Test GPU.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../TensorFlow/Optimizations/Train, Optimize, and Deploy Tensorflow AI Model.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\r\n",
      "\t\u001b[31m../../Conferences/StartupML/Jan-20-2017/checkpoint/\u001b[m\r\n",
      "\t\u001b[31m../../TensorFlow/Optimizations/checkpoint/\u001b[m\r\n",
      "\t\u001b[31m../../TensorFlow/Optimizations/model/\u001b[m\r\n",
      "\t\u001b[31m../../../scripts/pi/\u001b[m\r\n",
      "\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!git add --all ~/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is up-to-date with 'origin/master'.\r\n",
      "\r\n",
      "Changes to be committed:\r\n",
      "  (use \"git reset HEAD <file>...\" to unstage)\r\n",
      "\r\n",
      "\t\u001b[32mnew file:   ../../../scripts/pi/pi.py\u001b[m\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n",
      "\r\n",
      "\t\u001b[31mmodified:   ../../Conferences/StartupML/Jan-20-2017/SparkMLTensorflowAI-HybridCloud-ContinuousDeployment-Azure.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../Scikit-Learn/DecisionTreeIris.pmml\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../Scikit-Learn/Scikit-Learn-PMML-Deploy.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   PySpark - Calculate Pi.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../TensorFlow/DeepDream/deepdream.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../TensorFlow/GPU/Test GPU.ipynb\u001b[m\r\n",
      "\t\u001b[31mmodified:   ../../TensorFlow/Optimizations/Train, Optimize, and Deploy Tensorflow AI Model.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\r\n",
      "\t\u001b[31m../../Conferences/StartupML/Jan-20-2017/checkpoint/\u001b[m\r\n",
      "\t\u001b[31m../../TensorFlow/Optimizations/checkpoint/\u001b[m\r\n",
      "\t\u001b[31m../../TensorFlow/Optimizations/model/\u001b[m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 48dd592] updated pyspark scripts\r\n",
      " 1 file changed, 1 insertion(+), 4 deletions(-)\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"updated pyspark scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is ahead of 'origin/master' by 1 commit.\r\n",
      "  (use \"git push\" to publish your local commits)\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n",
      "\r\n",
      "\t\u001b[31mmodified:   PySpark - Calculate Pi.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\r\n",
      "\t\u001b[31m../../Conferences/StartupML/Jan-20-2017/checkpoint/\u001b[m\r\n",
      "\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: push.default is unset; its implicit value is changing in\n",
      "Git 2.0 from 'matching' to 'simple'. To squelch this message\n",
      "and maintain the current behavior after the default changes, use:\n",
      "\n",
      "  git config --global push.default matching\n",
      "\n",
      "To squelch this message and adopt the new behavior now, use:\n",
      "\n",
      "  git config --global push.default simple\n",
      "\n",
      "When push.default is set to 'matching', git will push local branches\n",
      "to the remote branches that already exist with the same name.\n",
      "\n",
      "In Git 2.0, Git will default to the more conservative 'simple'\n",
      "behavior, which only pushes the current branch to the corresponding\n",
      "remote branch that 'git pull' uses to update the current branch.\n",
      "\n",
      "See 'git help config' and search for 'push.default' for further information.\n",
      "(the 'simple' mode was introduced in Git 1.7.11. Use the similar mode\n",
      "'current' instead of 'simple' if you sometimes use older versions of Git)\n",
      "\n",
      "Permission denied (publickey).\n",
      "fatal: Could not read from remote repository.\n",
      "\n",
      "Please make sure you have the correct access rights\n",
      "and the repository exists.\n"
     ]
    }
   ],
   "source": [
    "# If this fails with \"Permission denied\", use terminal within jupyter to manually `git push`\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Airflow Workflow](http://demo.pipeline.io:8080/admin/) Deploys New PySpark Job through Github Post-Commit [Webhook](https://github.com/fluxcapacitor/pipeline/blob/master/scheduler.ml/airflow/github_webhook) to Triggers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe width=100% height=500px src=\"http://demo.pipeline.io:8080/admin\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "html = '<iframe width=100% height=500px src=\"http://demo.pipeline.io:8080/admin\">'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
