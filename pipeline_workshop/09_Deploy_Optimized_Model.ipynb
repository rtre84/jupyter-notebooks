{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Fully Optimized Model to TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT:  You Must STOP All Kernels and Terminal Session\n",
    "The GPU is wedged at this point.  We need to set it free!!\n",
    "\n",
    "![Shutdown All Kernels and Terminals](http://pipeline.io/img/shutdown-all-kernels-and-terminals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze Fully Optimized Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import freeze_graph\n",
    "\n",
    "optimize_me_parent_path = '/root/models/optimize_me/linear/cpu'\n",
    "\n",
    "fully_optimized_model_graph_path = '%s/fully_optimized_cpu.pb' % optimize_me_parent_path\n",
    "fully_optimized_frozen_model_graph_path = '%s/fully_optimized_frozen_cpu.pb' % optimize_me_parent_path\n",
    "\n",
    "model_checkpoint_path = '%s/model.ckpt' % optimize_me_parent_path\n",
    "\n",
    "freeze_graph.freeze_graph(input_graph=fully_optimized_model_graph_path, \n",
    "                          input_saver=\"\",\n",
    "                          input_binary=True, \n",
    "                          input_checkpoint='/root/models/optimize_me/linear/cpu/model.ckpt',\n",
    "                          output_node_names=\"add\",\n",
    "                          restore_op_name=\"save/restore_all\", \n",
    "                          filename_tensor_name=\"save/Const:0\",\n",
    "                          output_graph=fully_optimized_frozen_model_graph_path, \n",
    "                          clear_devices=True, \n",
    "                          initializer_nodes=\"\")\n",
    "print(fully_optimized_frozen_model_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -l /root/models/optimize_me/linear/cpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "summarize_graph --in_graph=/root/models/optimize_me/linear/cpu/fully_optimized_frozen_cpu.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import re\n",
    "from google.protobuf import text_format\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "\n",
    "def convert_graph_to_dot(input_graph, output_dot, is_input_graph_binary):\n",
    "    graph = graph_pb2.GraphDef()\n",
    "    with open(input_graph, \"rb\") as fh:\n",
    "        if is_input_graph_binary:\n",
    "            graph.ParseFromString(fh.read())\n",
    "        else:\n",
    "            text_format.Merge(fh.read(), graph)\n",
    "    with open(output_dot, \"wt\") as fh:\n",
    "        print(\"digraph graphname {\", file=fh)\n",
    "        for node in graph.node:\n",
    "            output_name = node.name\n",
    "            print(\"  \\\"\" + output_name + \"\\\" [label=\\\"\" + node.op + \"\\\"];\", file=fh)\n",
    "            for input_full_name in node.input:\n",
    "                parts = input_full_name.split(\":\")\n",
    "                input_name = re.sub(r\"^\\^\", \"\", parts[0])\n",
    "                print(\"  \\\"\" + input_name + \"\\\" -> \\\"\" + output_name + \"\\\";\", file=fh)\n",
    "        print(\"}\", file=fh)\n",
    "        print(\"Created dot file '%s' for graph '%s'.\" % (output_dot, input_graph))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph='/root/models/optimize_me/linear/cpu/fully_optimized_frozen_cpu.pb'\n",
    "output_dot='/root/notebooks/fully_optimized_frozen_cpu.dot'\n",
    "convert_graph_to_dot(input_graph=input_graph, output_dot=output_dot, is_input_graph_binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dot -T png /root/notebooks/fully_optimized_frozen_cpu.dot \\\n",
    "    -o /root/notebooks/fully_optimized_frozen_cpu.png > /tmp/a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('/root/notebooks/fully_optimized_frozen_cpu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Standalone Benchmarks\n",
    "Note:  These benchmarks are running against the standalone models on disk.  We will benchmark the models running within TensorFlow Serving soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "benchmark_model --graph=/root/models/optimize_me/linear/cpu/fully_optimized_frozen_cpu.pb \\\n",
    "    --input_layer=weights,bias,x_observed \\\n",
    "    --input_layer_type=float,float,float \\\n",
    "    --input_layer_shape=:: \\\n",
    "    --output_layer=add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model for Deployment and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Default Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Version Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "version = int(datetime.now().strftime(\"%s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Optimized, Frozen Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "inspect_checkpoint --file_name=/root/models/optimize_me/linear/cpu/model.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph('/root/models/optimize_me/linear/cpu/model.ckpt.meta')\n",
    "saver.restore(sess, '/root/models/optimize_me/linear/cpu/model.ckpt')\n",
    "\n",
    "optimize_me_parent_path = '/root/models/optimize_me/linear/cpu'\n",
    "fully_optimized_frozen_model_graph_path = '%s/fully_optimized_frozen_cpu.pb' % optimize_me_parent_path\n",
    "print(fully_optimized_frozen_model_graph_path)\n",
    "\n",
    "with tf.gfile.GFile(fully_optimized_frozen_model_graph_path, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "tf.import_graph_def(\n",
    "    graph_def, \n",
    "    input_map=None, \n",
    "    return_elements=None, \n",
    "    name=\"\", \n",
    "    op_dict=None, \n",
    "    producer_op_list=None\n",
    ")\n",
    "\n",
    "print(\"weights = \", sess.run(\"weights:0\"))\n",
    "print(\"bias = \", sess.run(\"bias:0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `SignatureDef` Asset for TensorFlow Serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "x_observed = graph.get_tensor_by_name('x_observed:0')\n",
    "y_pred = graph.get_tensor_by_name('add:0')\n",
    "\n",
    "inputs_map = {'inputs': x_observed}\n",
    "outputs_map = {'outputs': y_pred}\n",
    "\n",
    "predict_signature = signature_def_utils.predict_signature_def(\n",
    "                inputs = inputs_map, \n",
    "                outputs = outputs_map)\n",
    "print(predict_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model with Assets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "fully_optimized_saved_model_path = '/root/models/linear_fully_optimized/cpu/%s' % version\n",
    "print(fully_optimized_saved_model_path)\n",
    "\n",
    "builder = saved_model_builder.SavedModelBuilder(fully_optimized_saved_model_path)\n",
    "builder.add_meta_graph_and_variables(sess, \n",
    "                                     [tag_constants.SERVING],\n",
    "                                     signature_def_map={'predict':predict_signature,                                     \n",
    "signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:predict_signature}, \n",
    "                                     clear_devices=True,\n",
    ")\n",
    "\n",
    "builder.save(as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(fully_optimized_saved_model_path)\n",
    "os.listdir(fully_optimized_saved_model_path)\n",
    "os.listdir('%s/variables' % fully_optimized_saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect with [Saved Model CLI](https://www.tensorflow.org/programmers_guide/saved_model_cli)\n",
    "Note:  This takes a minute or two for some reason.  Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "output = subprocess.run([\"saved_model_cli\", \"show\", \\\n",
    "                \"--dir\", fully_optimized_saved_model_path, \"--all\"], \\\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE)\n",
    "\n",
    "print(output.stdout.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a Terminal through Jupyter Notebook \n",
    "### (Menu Bar -> File -> New...)\n",
    "\n",
    "![Jupyter Terminal](http://pipeline.io/img/jupyter-terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Http-Grpc Proxy in Separate Terminal\n",
    "```\n",
    "http_grpc_proxy 9004 9000\n",
    "```\n",
    "\n",
    "The params are as follows:\n",
    "* 1: `proxy_port` for this proxy\n",
    "* 2: `tf_serving_port` for TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start TensorFlow Serving in Separate Terminal\n",
    "Point to the `model_base_path` of the fully optimized model.  \n",
    "\n",
    "The params are as follows:\n",
    "* `port` (int)\n",
    "* `model_name` (anything)\n",
    "* `model_base_path` (/path/to/model/ above all versioned sub-directories)\n",
    "* `enable_batching` (true|false)\n",
    "\n",
    "```\n",
    "tensorflow_model_server \\\n",
    "  --port=9000 \\\n",
    "  --model_name=linear \\\n",
    "  --model_base_path=/root/models/linear_fully_optimized/cpu \\\n",
    "  --enable_batching=false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Following Command in the Terminal to Predict\n",
    "The params are as follows:\n",
    "* 1: `proxy_port`\n",
    "* 2: `x_observed` feed input\n",
    "\n",
    "Returns:\n",
    "* `y_pred` prediction\n",
    "\n",
    "```\n",
    "predict 9004 1.5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor GPU in Separate Terminal \n",
    "```\n",
    "watch -n 1 nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Load Test in Separate Terminal\n",
    "The params are as follows:\n",
    "* `$1: amount of load low|medium|high`\n",
    "\n",
    "```\n",
    "loadtest high\n",
    "\n",
    "### EXPECTED OUTPUT ###\n",
    "summary ... 38.6/s Avg:  1175 Min:    26 Max:  2320 Err:     0 (0.00%)\n",
    "summary ... 37.3/s Avg:  2586 Min:  2331 Max:  2729 Err:     0 (0.00%)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
